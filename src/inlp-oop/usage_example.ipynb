{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import inlp_dataset_handler\n",
    "import inlp\n",
    "import inlp_linear_model\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev = np.random.rand(1000,100) - 0.5, np.random.rand(1000,100) - 0.5\n",
    "y_train, y_dev = np.sum(x_train, axis = 1) > 0, np.sum(x_dev, axis = 1) > 0\n",
    "\n",
    "inlp_dataset = inlp_dataset_handler.ClassificationDatasetHandler(x_train, y_train, x_dev, y_dev, dropout_rate = 0, Y_train_main = None, Y_dev_main = None, by_class = False, equal_chance_for_main_task_labels = False)\n",
    "\n",
    "inlp_model_handler = inlp_linear_model.SKlearnClassifier(LinearSVC, {\"dual\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "iteration: 0, accuracy: 0.944:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "iteration: 1, accuracy: 0.49:   0%|          | 0/5 [00:00<?, ?it/s] \u001b[A\n",
      "iteration: 1, accuracy: 0.49:  40%|████      | 2/5 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "iteration: 2, accuracy: 0.507:  40%|████      | 2/5 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "iteration: 3, accuracy: 0.518:  40%|████      | 2/5 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "iteration: 3, accuracy: 0.518:  80%|████████  | 4/5 [00:00<00:00, 14.20it/s]\u001b[A\n",
      "iteration: 4, accuracy: 0.541: 100%|██████████| 5/5 [00:00<00:00, 13.68it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "P, rowspace_projections, Ws = inlp.run_INLP(num_classifiers = 5, input_dim = 100, is_autoregressive = True, min_accuracy = 0, dataset_handler = inlp_dataset, model = inlp_model_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sanity_check(P, Ws, x_train):\n",
    "\n",
    "    assert np.allclose(P.dot(P), P)\n",
    "    assert np.allclose(Ws[0].dot(P.dot(x_train[0])), 0.0)\n",
    "\n",
    "    for w in Ws:\n",
    "        for w2 in Ws:\n",
    "            if w is w2: continue\n",
    "            assert np.allclose(w.dot(w2.T).item(), 0.0)\n",
    "            \n",
    "do_sanity_check(P, Ws, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_train2 = np.random.rand(1000,100) - 0.5,  np.random.rand(1000,100) - 0.5,\n",
    "x_dev1, x_dev2 =  np.random.rand(1000,100) - 0.5, np.random.rand(1000,100) - 0.5\n",
    "\n",
    "y_train = (np.sign(np.sum(x_train1, axis = 1)) ==  np.sign(np.sum(x_train2, axis = 1))).astype(int)\n",
    "y_dev = (np.sign(np.sum(x_dev1, axis = 1)) ==  np.sign(np.sum(x_dev2, axis = 1))).astype(int)\n",
    "\n",
    "inlp_dataset = inlp_dataset_handler.SiameseDatasetHandler((x_train1, x_train2), y_train, (x_dev1, x_dev2), y_dev, dropout_rate = 0, Y_train_main = None, Y_dev_main = None, by_class = False, equal_chance_for_main_task_labels = False)\n",
    "params = {\"num_iter\": 25, \"input_dim\": 100, \"hidden_dim\": 32, \"batch_size\": 64}\n",
    "inlp_model_handler = inlp_linear_model.SiameseLinearClassifier(model_params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[AINFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1          l2             Linear    3 K\n",
      "2  cosine_sim   CosineSimilarity    0  \n",
      "3     loss_fn  BCEWithLogitsLoss    0  \n",
      "Validation sanity check:   0%|          | 0/5 [00:00<?, ?batch/s]\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 99.54batch/s, batch_nb=15, loss=0.698, v_nb=70]\n",
      "Epoch 1:  72%|███████▏  | 23/32 [00:00<00:00, 106.81batch/s, batch_nb=15, loss=0.698, v_nb=70]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 106.81batch/s, batch_nb=15, loss=0.698, v_nb=70]\n",
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 96.56batch/s, batch_nb=15, loss=0.686, v_nb=70] \n",
      "Epoch 2:  62%|██████▎   | 20/32 [00:00<00:00, 102.22batch/s, batch_nb=15, loss=0.686, v_nb=70]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 102.22batch/s, batch_nb=15, loss=0.686, v_nb=70]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 101.26batch/s, batch_nb=15, loss=0.677, v_nb=70]\n",
      "Epoch 3:  66%|██████▌   | 21/32 [00:00<00:00, 103.33batch/s, batch_nb=15, loss=0.677, v_nb=70]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 103.33batch/s, batch_nb=15, loss=0.677, v_nb=70]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 103.64batch/s, batch_nb=15, loss=0.668, v_nb=70]\n",
      "Epoch 4:  72%|███████▏  | 23/32 [00:00<00:00, 107.83batch/s, batch_nb=15, loss=0.668, v_nb=70]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 107.83batch/s, batch_nb=15, loss=0.668, v_nb=70]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 106.41batch/s, batch_nb=15, loss=0.659, v_nb=70]\n",
      "Epoch 5:  75%|███████▌  | 24/32 [00:00<00:00, 111.53batch/s, batch_nb=15, loss=0.659, v_nb=70]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 111.53batch/s, batch_nb=15, loss=0.659, v_nb=70]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 106.52batch/s, batch_nb=15, loss=0.651, v_nb=70]\n",
      "Epoch 6:  75%|███████▌  | 24/32 [00:00<00:00, 111.14batch/s, batch_nb=15, loss=0.651, v_nb=70]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 111.14batch/s, batch_nb=15, loss=0.651, v_nb=70]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 104.19batch/s, batch_nb=15, loss=0.637, v_nb=70]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 113.45batch/s, batch_nb=15, loss=0.637, v_nb=70]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 108.13batch/s, batch_nb=15, loss=0.620, v_nb=70]\n",
      "Epoch 8:  78%|███████▊  | 25/32 [00:00<00:00, 112.46batch/s, batch_nb=15, loss=0.620, v_nb=70]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 112.46batch/s, batch_nb=15, loss=0.620, v_nb=70]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 104.67batch/s, batch_nb=15, loss=0.605, v_nb=70]\n",
      "Epoch 9:  81%|████████▏ | 26/32 [00:00<00:00, 113.15batch/s, batch_nb=15, loss=0.605, v_nb=70]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 113.15batch/s, batch_nb=15, loss=0.605, v_nb=70]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 103.94batch/s, batch_nb=15, loss=0.591, v_nb=70]\n",
      "Epoch 10:  84%|████████▍ | 27/32 [00:00<00:00, 113.16batch/s, batch_nb=15, loss=0.591, v_nb=70]\n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 113.16batch/s, batch_nb=15, loss=0.591, v_nb=70]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 103.08batch/s, batch_nb=15, loss=0.577, v_nb=70]\n",
      "Epoch 11:  81%|████████▏ | 26/32 [00:00<00:00, 110.00batch/s, batch_nb=15, loss=0.577, v_nb=70]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 110.00batch/s, batch_nb=15, loss=0.577, v_nb=70]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 97.59batch/s, batch_nb=15, loss=0.563, v_nb=70] \n",
      "Validating:   0%|          | 0/16 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 88.17batch/s, batch_nb=15, loss=0.563, v_nb=70]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 84.65batch/s, batch_nb=15, loss=0.550, v_nb=70]\n",
      "Epoch 13:  59%|█████▉    | 19/32 [00:00<00:00, 81.94batch/s, batch_nb=15, loss=0.550, v_nb=70]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 83.70batch/s, batch_nb=15, loss=0.550, v_nb=70]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 85.27batch/s, batch_nb=15, loss=0.538, v_nb=70]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 94.08batch/s, batch_nb=15, loss=0.538, v_nb=70]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 94.10batch/s, batch_nb=15, loss=0.526, v_nb=70]\n",
      "Epoch 15:  69%|██████▉   | 22/32 [00:00<00:00, 98.37batch/s, batch_nb=15, loss=0.526, v_nb=70]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 98.37batch/s, batch_nb=15, loss=0.526, v_nb=70]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 96.94batch/s, batch_nb=15, loss=0.515, v_nb=70]\n",
      "Epoch 16:  78%|███████▊  | 25/32 [00:00<00:00, 103.55batch/s, batch_nb=15, loss=0.515, v_nb=70]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 103.55batch/s, batch_nb=15, loss=0.515, v_nb=70]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 100.05batch/s, batch_nb=15, loss=0.505, v_nb=70]\n",
      "Epoch 17:  72%|███████▏  | 23/32 [00:00<00:00, 104.42batch/s, batch_nb=15, loss=0.505, v_nb=70]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 104.42batch/s, batch_nb=15, loss=0.505, v_nb=70]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 101.49batch/s, batch_nb=15, loss=0.494, v_nb=70]\n",
      "Epoch 18:  72%|███████▏  | 23/32 [00:00<00:00, 105.83batch/s, batch_nb=15, loss=0.494, v_nb=70]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 105.83batch/s, batch_nb=15, loss=0.494, v_nb=70]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 101.86batch/s, batch_nb=15, loss=0.484, v_nb=70]\n",
      "Epoch 19:  78%|███████▊  | 25/32 [00:00<00:00, 109.02batch/s, batch_nb=15, loss=0.484, v_nb=70]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 109.02batch/s, batch_nb=15, loss=0.484, v_nb=70]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 106.08batch/s, batch_nb=15, loss=0.475, v_nb=70]\n",
      "Epoch 20:  78%|███████▊  | 25/32 [00:00<00:00, 111.69batch/s, batch_nb=15, loss=0.475, v_nb=70]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 111.69batch/s, batch_nb=15, loss=0.475, v_nb=70]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 107.39batch/s, batch_nb=15, loss=0.466, v_nb=70]\n",
      "Epoch 21:  81%|████████▏ | 26/32 [00:00<00:00, 115.02batch/s, batch_nb=15, loss=0.466, v_nb=70]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 115.02batch/s, batch_nb=15, loss=0.466, v_nb=70]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 105.70batch/s, batch_nb=15, loss=0.457, v_nb=70]\n",
      "Epoch 22:  84%|████████▍ | 27/32 [00:00<00:00, 114.18batch/s, batch_nb=15, loss=0.457, v_nb=70]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 114.18batch/s, batch_nb=15, loss=0.457, v_nb=70]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 107.20batch/s, batch_nb=15, loss=0.448, v_nb=70]\n",
      "Epoch 23:  81%|████████▏ | 26/32 [00:00<00:00, 113.78batch/s, batch_nb=15, loss=0.448, v_nb=70]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 113.78batch/s, batch_nb=15, loss=0.448, v_nb=70]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 107.13batch/s, batch_nb=15, loss=0.440, v_nb=70]\n",
      "Epoch 24:  78%|███████▊  | 25/32 [00:00<00:00, 112.54batch/s, batch_nb=15, loss=0.440, v_nb=70]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 112.54batch/s, batch_nb=15, loss=0.440, v_nb=70]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 104.26batch/s, batch_nb=15, loss=0.432, v_nb=70]\n",
      "Epoch 25:  72%|███████▏  | 23/32 [00:00<00:00, 99.22batch/s, batch_nb=15, loss=0.432, v_nb=70] \n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 99.22batch/s, batch_nb=15, loss=0.432, v_nb=70]\n",
      "                                                             \u001b[A\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 86.54batch/s, batch_nb=15, loss=0.432, v_nb=70]\n",
      "iteration: 0, accuracy: 0.658984363079071:  33%|███▎      | 1/3 [00:07<00:14,  7.38s/it]INFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1          l2             Linear    3 K\n",
      "2  cosine_sim   CosineSimilarity    0  \n",
      "3     loss_fn  BCEWithLogitsLoss    0  \n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 94.29batch/s, batch_nb=15, loss=0.692, v_nb=71]\n",
      "Epoch 1:  59%|█████▉    | 19/32 [00:00<00:00, 91.34batch/s, batch_nb=15, loss=0.692, v_nb=71]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 93.18batch/s, batch_nb=15, loss=0.692, v_nb=71]\n",
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 87.11batch/s, batch_nb=15, loss=0.689, v_nb=71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  62%|██████▎   | 20/32 [00:00<00:00, 94.50batch/s, batch_nb=15, loss=0.689, v_nb=71]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 94.50batch/s, batch_nb=15, loss=0.689, v_nb=71]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 86.04batch/s, batch_nb=15, loss=0.685, v_nb=71]\n",
      "Epoch 3:  59%|█████▉    | 19/32 [00:00<00:00, 88.57batch/s, batch_nb=15, loss=0.685, v_nb=71]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 88.57batch/s, batch_nb=15, loss=0.685, v_nb=71]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 90.62batch/s, batch_nb=15, loss=0.682, v_nb=71]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 101.68batch/s, batch_nb=15, loss=0.682, v_nb=71]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 102.86batch/s, batch_nb=15, loss=0.679, v_nb=71]\n",
      "Epoch 5:  75%|███████▌  | 24/32 [00:00<00:00, 108.48batch/s, batch_nb=15, loss=0.679, v_nb=71]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 108.48batch/s, batch_nb=15, loss=0.679, v_nb=71]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 106.44batch/s, batch_nb=15, loss=0.675, v_nb=71]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 117.09batch/s, batch_nb=15, loss=0.675, v_nb=71]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 112.65batch/s, batch_nb=15, loss=0.670, v_nb=71]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 122.24batch/s, batch_nb=15, loss=0.670, v_nb=71]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 113.94batch/s, batch_nb=15, loss=0.663, v_nb=71]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 123.67batch/s, batch_nb=15, loss=0.663, v_nb=71]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 100.95batch/s, batch_nb=15, loss=0.655, v_nb=71]\n",
      "Epoch 9:  78%|███████▊  | 25/32 [00:00<00:00, 102.32batch/s, batch_nb=15, loss=0.655, v_nb=71]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 102.32batch/s, batch_nb=15, loss=0.655, v_nb=71]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 97.36batch/s, batch_nb=15, loss=0.648, v_nb=71]\n",
      "Epoch 10:  72%|███████▏  | 23/32 [00:00<00:00, 102.88batch/s, batch_nb=15, loss=0.648, v_nb=71]\n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 102.88batch/s, batch_nb=15, loss=0.648, v_nb=71]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 103.89batch/s, batch_nb=15, loss=0.641, v_nb=71]\n",
      "Epoch 11:  78%|███████▊  | 25/32 [00:00<00:00, 111.56batch/s, batch_nb=15, loss=0.641, v_nb=71]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 111.56batch/s, batch_nb=15, loss=0.641, v_nb=71]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 106.49batch/s, batch_nb=15, loss=0.634, v_nb=71]\n",
      "Epoch 12:  81%|████████▏ | 26/32 [00:00<00:00, 113.69batch/s, batch_nb=15, loss=0.634, v_nb=71]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 113.69batch/s, batch_nb=15, loss=0.634, v_nb=71]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 107.64batch/s, batch_nb=15, loss=0.627, v_nb=71]\n",
      "Epoch 13:  78%|███████▊  | 25/32 [00:00<00:00, 111.58batch/s, batch_nb=15, loss=0.627, v_nb=71]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 111.58batch/s, batch_nb=15, loss=0.627, v_nb=71]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 105.37batch/s, batch_nb=15, loss=0.621, v_nb=71]\n",
      "Epoch 14:  81%|████████▏ | 26/32 [00:00<00:00, 113.32batch/s, batch_nb=15, loss=0.621, v_nb=71]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 113.32batch/s, batch_nb=15, loss=0.621, v_nb=71]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 107.70batch/s, batch_nb=15, loss=0.614, v_nb=71]\n",
      "Epoch 15:  81%|████████▏ | 26/32 [00:00<00:00, 115.20batch/s, batch_nb=15, loss=0.614, v_nb=71]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 115.20batch/s, batch_nb=15, loss=0.614, v_nb=71]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 109.46batch/s, batch_nb=15, loss=0.608, v_nb=71]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 117.98batch/s, batch_nb=15, loss=0.608, v_nb=71]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 95.40batch/s, batch_nb=15, loss=0.602, v_nb=71] \n",
      "Epoch 17:  88%|████████▊ | 28/32 [00:00<00:00, 104.86batch/s, batch_nb=15, loss=0.602, v_nb=71]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 104.86batch/s, batch_nb=15, loss=0.602, v_nb=71]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 101.21batch/s, batch_nb=15, loss=0.596, v_nb=71]\n",
      "Epoch 18:  78%|███████▊  | 25/32 [00:00<00:00, 108.11batch/s, batch_nb=15, loss=0.596, v_nb=71]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 108.11batch/s, batch_nb=15, loss=0.596, v_nb=71]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 104.18batch/s, batch_nb=15, loss=0.590, v_nb=71]\n",
      "Epoch 19:  78%|███████▊  | 25/32 [00:00<00:00, 110.58batch/s, batch_nb=15, loss=0.590, v_nb=71]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 110.58batch/s, batch_nb=15, loss=0.590, v_nb=71]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 105.02batch/s, batch_nb=15, loss=0.584, v_nb=71]\n",
      "Epoch 20:  78%|███████▊  | 25/32 [00:00<00:00, 111.40batch/s, batch_nb=15, loss=0.584, v_nb=71]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 111.40batch/s, batch_nb=15, loss=0.584, v_nb=71]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 104.76batch/s, batch_nb=15, loss=0.579, v_nb=71]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 114.29batch/s, batch_nb=15, loss=0.579, v_nb=71]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 108.33batch/s, batch_nb=15, loss=0.574, v_nb=71]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 118.00batch/s, batch_nb=15, loss=0.574, v_nb=71]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 110.62batch/s, batch_nb=15, loss=0.569, v_nb=71]\n",
      "Epoch 23:  88%|████████▊ | 28/32 [00:00<00:00, 118.34batch/s, batch_nb=15, loss=0.569, v_nb=71]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 118.34batch/s, batch_nb=15, loss=0.569, v_nb=71]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 110.75batch/s, batch_nb=15, loss=0.564, v_nb=71]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 120.98batch/s, batch_nb=15, loss=0.564, v_nb=71]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 110.05batch/s, batch_nb=15, loss=0.559, v_nb=71]\n",
      "Epoch 25:  84%|████████▍ | 27/32 [00:00<00:00, 116.88batch/s, batch_nb=15, loss=0.559, v_nb=71]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 116.88batch/s, batch_nb=15, loss=0.559, v_nb=71]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 112.98batch/s, batch_nb=15, loss=0.559, v_nb=71]\n",
      "iteration: 1, accuracy: 0.5162109136581421:  67%|██████▋   | 2/3 [00:14<00:07,  7.31s/it]INFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1          l2             Linear    3 K\n",
      "2  cosine_sim   CosineSimilarity    0  \n",
      "3     loss_fn  BCEWithLogitsLoss    0  \n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 97.12batch/s, batch_nb=15, loss=0.696, v_nb=72]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 104.44batch/s, batch_nb=15, loss=0.696, v_nb=72]\n",
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 102.60batch/s, batch_nb=15, loss=0.685, v_nb=72]\n",
      "Epoch 2:  69%|██████▉   | 22/32 [00:00<00:00, 107.25batch/s, batch_nb=15, loss=0.685, v_nb=72]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 107.25batch/s, batch_nb=15, loss=0.685, v_nb=72]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 104.82batch/s, batch_nb=15, loss=0.677, v_nb=72]\n",
      "Epoch 3:  69%|██████▉   | 22/32 [00:00<00:00, 105.68batch/s, batch_nb=15, loss=0.677, v_nb=72]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 105.68batch/s, batch_nb=15, loss=0.677, v_nb=72]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 100.96batch/s, batch_nb=15, loss=0.669, v_nb=72]\n",
      "Epoch 4:  66%|██████▌   | 21/32 [00:00<00:00, 101.76batch/s, batch_nb=15, loss=0.669, v_nb=72]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 101.76batch/s, batch_nb=15, loss=0.669, v_nb=72]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 98.87batch/s, batch_nb=15, loss=0.661, v_nb=72] \n",
      "Epoch 5:  66%|██████▌   | 21/32 [00:00<00:00, 101.63batch/s, batch_nb=15, loss=0.661, v_nb=72]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 101.63batch/s, batch_nb=15, loss=0.661, v_nb=72]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 95.91batch/s, batch_nb=15, loss=0.654, v_nb=72] \n",
      "Epoch 6:  69%|██████▉   | 22/32 [00:00<00:00, 101.25batch/s, batch_nb=15, loss=0.654, v_nb=72]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 101.25batch/s, batch_nb=15, loss=0.654, v_nb=72]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 98.07batch/s, batch_nb=15, loss=0.641, v_nb=72] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  72%|███████▏  | 23/32 [00:00<00:00, 103.61batch/s, batch_nb=15, loss=0.641, v_nb=72]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 103.61batch/s, batch_nb=15, loss=0.641, v_nb=72]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 101.76batch/s, batch_nb=15, loss=0.628, v_nb=72]\n",
      "Epoch 8:  72%|███████▏  | 23/32 [00:00<00:00, 106.47batch/s, batch_nb=15, loss=0.628, v_nb=72]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 106.47batch/s, batch_nb=15, loss=0.628, v_nb=72]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 107.31batch/s, batch_nb=15, loss=0.616, v_nb=72]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 116.22batch/s, batch_nb=15, loss=0.616, v_nb=72]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 112.34batch/s, batch_nb=15, loss=0.604, v_nb=72]\n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 121.82batch/s, batch_nb=15, loss=0.604, v_nb=72]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 116.00batch/s, batch_nb=15, loss=0.594, v_nb=72]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 125.56batch/s, batch_nb=15, loss=0.594, v_nb=72]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 117.83batch/s, batch_nb=15, loss=0.584, v_nb=72]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 128.16batch/s, batch_nb=15, loss=0.584, v_nb=72]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 119.13batch/s, batch_nb=15, loss=0.575, v_nb=72]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 128.45batch/s, batch_nb=15, loss=0.575, v_nb=72]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 115.67batch/s, batch_nb=15, loss=0.567, v_nb=72]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 127.02batch/s, batch_nb=15, loss=0.567, v_nb=72]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 111.62batch/s, batch_nb=15, loss=0.560, v_nb=72]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 120.74batch/s, batch_nb=15, loss=0.560, v_nb=72]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 114.98batch/s, batch_nb=15, loss=0.553, v_nb=72]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 124.10batch/s, batch_nb=15, loss=0.553, v_nb=72]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 117.14batch/s, batch_nb=15, loss=0.547, v_nb=72]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 126.44batch/s, batch_nb=15, loss=0.547, v_nb=72]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 118.23batch/s, batch_nb=15, loss=0.542, v_nb=72]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 129.27batch/s, batch_nb=15, loss=0.542, v_nb=72]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 119.27batch/s, batch_nb=15, loss=0.536, v_nb=72]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 129.81batch/s, batch_nb=15, loss=0.536, v_nb=72]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 122.33batch/s, batch_nb=15, loss=0.532, v_nb=72]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 131.92batch/s, batch_nb=15, loss=0.532, v_nb=72]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 122.18batch/s, batch_nb=15, loss=0.528, v_nb=72]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 132.41batch/s, batch_nb=15, loss=0.528, v_nb=72]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 118.21batch/s, batch_nb=15, loss=0.524, v_nb=72]\n",
      "Epoch 22:  88%|████████▊ | 28/32 [00:00<00:00, 122.05batch/s, batch_nb=15, loss=0.524, v_nb=72]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 122.05batch/s, batch_nb=15, loss=0.524, v_nb=72]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 113.29batch/s, batch_nb=15, loss=0.521, v_nb=72]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 123.13batch/s, batch_nb=15, loss=0.521, v_nb=72]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 111.38batch/s, batch_nb=15, loss=0.517, v_nb=72]\n",
      "Epoch 24:  84%|████████▍ | 27/32 [00:00<00:00, 116.25batch/s, batch_nb=15, loss=0.517, v_nb=72]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 116.25batch/s, batch_nb=15, loss=0.517, v_nb=72]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 106.92batch/s, batch_nb=15, loss=0.514, v_nb=72]\n",
      "Epoch 25:  78%|███████▊  | 25/32 [00:00<00:00, 109.07batch/s, batch_nb=15, loss=0.514, v_nb=72]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 109.07batch/s, batch_nb=15, loss=0.514, v_nb=72]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 104.78batch/s, batch_nb=15, loss=0.514, v_nb=72]\n",
      "iteration: 2, accuracy: 0.5083984136581421: 100%|██████████| 3/3 [00:21<00:00,  7.10s/it]\n"
     ]
    }
   ],
   "source": [
    "#inlp_model_handler.train_model(inlp_dataset)\n",
    "P, rowspace_projections, Ws = inlp.run_INLP(num_classifiers = 3, input_dim = 100, is_autoregressive = True, min_accuracy = 0, dataset_handler = inlp_dataset, model = inlp_model_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note that the cosine/l2 distance loss is no longer convex, so w_i.dot(w_j) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02524728, -0.0030968 ,  0.10105404, ..., -0.04164061,\n",
       "        -0.04635901, -0.04721296],\n",
       "       [-0.015494  , -0.0843562 , -0.04074406, ...,  0.04730713,\n",
       "         0.04409238, -0.03214298],\n",
       "       [-0.04279207,  0.04437556, -0.01076072, ..., -0.05580216,\n",
       "        -0.05694133, -0.00659285],\n",
       "       ...,\n",
       "       [ 0.02467375, -0.08991509, -0.14390472, ..., -0.00712462,\n",
       "         0.02172526,  0.12529522],\n",
       "       [-0.01994726, -0.00422698,  0.08100867, ..., -0.03052652,\n",
       "        -0.08544648, -0.03796824],\n",
       "       [-0.03969179,  0.02470817, -0.0938241 , ...,  0.02075064,\n",
       "         0.07615001, -0.08765589]], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[0].dot(Ws[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.442013957321746e-06"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(P.dot(P) - P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
