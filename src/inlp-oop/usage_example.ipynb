{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import inlp_dataset_handler\n",
    "import inlp\n",
    "import inlp_linear_model\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev = np.random.rand(1000,100) - 0.5, np.random.rand(1000,100) - 0.5\n",
    "y_train, y_dev = np.sum(x_train, axis = 1) > 0, np.sum(x_dev, axis = 1) > 0\n",
    "\n",
    "inlp_dataset = inlp_dataset_handler.ClassificationDatasetHandler(x_train, y_train, x_dev, y_dev, dropout_rate = 0, Y_train_main = None, Y_dev_main = None, by_class = False, equal_chance_for_main_task_labels = False)\n",
    "\n",
    "inlp_model_handler = inlp_linear_model.SKlearnClassifier(LinearSVC, {\"dual\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 4, accuracy: 0.506: 100%|██████████| 5/5 [00:00<00:00, 23.40it/s]\n"
     ]
    }
   ],
   "source": [
    "P, rowspace_projections, Ws = inlp.run_INLP(num_classifiers = 5, input_dim = 100, is_autoregressive = True, min_accuracy = 0, dataset_handler = inlp_dataset, model = inlp_model_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sanity_check(P, Ws, x_train):\n",
    "\n",
    "    assert np.allclose(P.dot(P), P)\n",
    "    assert np.allclose(Ws[0].dot(P.dot(x_train[0])), 0.0)\n",
    "\n",
    "    for w in Ws:\n",
    "        for w2 in Ws:\n",
    "            if w is w2: continue\n",
    "            assert np.allclose(w.dot(w2.T).item(), 0.0)\n",
    "            \n",
    "do_sanity_check(P, Ws, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_train2 = np.random.rand(1000,100) - 0.5,  np.random.rand(1000,100) - 0.5,\n",
    "x_dev1, x_dev2 =  np.random.rand(1000,100) - 0.5, np.random.rand(1000,100) - 0.5\n",
    "\n",
    "y_train = (np.sign(np.sum(x_train1, axis = 1)) ==  np.sign(np.sum(x_train2, axis = 1))).astype(int)\n",
    "y_dev = (np.sign(np.sum(x_dev1, axis = 1)) ==  np.sign(np.sum(x_dev2, axis = 1))).astype(int)\n",
    "\n",
    "inlp_dataset = inlp_dataset_handler.SiameseDatasetHandler((x_train1, x_train2), y_train, (x_dev1, x_dev2), y_dev, dropout_rate = 0, Y_train_main = None, Y_dev_main = None, by_class = False, equal_chance_for_main_task_labels = False)\n",
    "params = {\"num_iter\": 25, \"input_dim\": 100, \"hidden_dim\": 32, \"batch_size\": 64, \"verbose\": True, \"device\": \"cuda\",\n",
    "         \"compare_by\": \"cosine\", \"same_weights\": True}\n",
    "inlp_model_handler = inlp_linear_model.SiameseLinearClassifier(model_params = params, concat_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[AINFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1  cosine_sim   CosineSimilarity    0  \n",
      "2     loss_fn  BCEWithLogitsLoss    0  \n",
      "Validation sanity check:   0%|          | 0/5 [00:00<?, ?batch/s]\n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 106.02batch/s, batch_nb=15, loss=0.694, v_nb=118]\n",
      "Epoch 1:  78%|███████▊  | 25/32 [00:00<00:00, 111.44batch/s, batch_nb=15, loss=0.694, v_nb=118]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 111.44batch/s, batch_nb=15, loss=0.694, v_nb=118]\n",
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 109.53batch/s, batch_nb=15, loss=0.684, v_nb=118]\n",
      "Epoch 2:  81%|████████▏ | 26/32 [00:00<00:00, 116.09batch/s, batch_nb=15, loss=0.684, v_nb=118]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 116.09batch/s, batch_nb=15, loss=0.684, v_nb=118]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 112.97batch/s, batch_nb=15, loss=0.676, v_nb=118]\n",
      "Epoch 3:  78%|███████▊  | 25/32 [00:00<00:00, 117.44batch/s, batch_nb=15, loss=0.676, v_nb=118]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 117.44batch/s, batch_nb=15, loss=0.676, v_nb=118]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 111.04batch/s, batch_nb=15, loss=0.668, v_nb=118]\n",
      "Epoch 4:  75%|███████▌  | 24/32 [00:00<00:00, 114.40batch/s, batch_nb=15, loss=0.668, v_nb=118]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 114.40batch/s, batch_nb=15, loss=0.668, v_nb=118]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 108.68batch/s, batch_nb=15, loss=0.661, v_nb=118]\n",
      "Epoch 5:  78%|███████▊  | 25/32 [00:00<00:00, 115.53batch/s, batch_nb=15, loss=0.661, v_nb=118]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 115.53batch/s, batch_nb=15, loss=0.661, v_nb=118]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 108.52batch/s, batch_nb=15, loss=0.654, v_nb=118]\n",
      "Epoch 6:  81%|████████▏ | 26/32 [00:00<00:00, 115.49batch/s, batch_nb=15, loss=0.654, v_nb=118]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 115.49batch/s, batch_nb=15, loss=0.654, v_nb=118]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 109.19batch/s, batch_nb=15, loss=0.642, v_nb=118]\n",
      "Epoch 7:  78%|███████▊  | 25/32 [00:00<00:00, 114.66batch/s, batch_nb=15, loss=0.642, v_nb=118]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 114.66batch/s, batch_nb=15, loss=0.642, v_nb=118]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 107.83batch/s, batch_nb=15, loss=0.628, v_nb=118]\n",
      "Epoch 8:  81%|████████▏ | 26/32 [00:00<00:00, 115.81batch/s, batch_nb=15, loss=0.628, v_nb=118]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 115.81batch/s, batch_nb=15, loss=0.628, v_nb=118]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 107.76batch/s, batch_nb=15, loss=0.616, v_nb=118]\n",
      "Epoch 9:  81%|████████▏ | 26/32 [00:00<00:00, 114.10batch/s, batch_nb=15, loss=0.616, v_nb=118]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 114.10batch/s, batch_nb=15, loss=0.616, v_nb=118]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 101.65batch/s, batch_nb=15, loss=0.604, v_nb=118]\n",
      "Epoch 10:  72%|███████▏  | 23/32 [00:00<00:00, 95.01batch/s, batch_nb=15, loss=0.604, v_nb=118] \n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 95.01batch/s, batch_nb=15, loss=0.604, v_nb=118]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 94.17batch/s, batch_nb=15, loss=0.593, v_nb=118]\n",
      "Epoch 11:  62%|██████▎   | 20/32 [00:00<00:00, 94.01batch/s, batch_nb=15, loss=0.593, v_nb=118]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 94.01batch/s, batch_nb=15, loss=0.593, v_nb=118]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 89.52batch/s, batch_nb=15, loss=0.583, v_nb=118]\n",
      "Epoch 12:  69%|██████▉   | 22/32 [00:00<00:00, 96.37batch/s, batch_nb=15, loss=0.583, v_nb=118]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 96.37batch/s, batch_nb=15, loss=0.583, v_nb=118]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 94.40batch/s, batch_nb=15, loss=0.572, v_nb=118]\n",
      "Epoch 13:  72%|███████▏  | 23/32 [00:00<00:00, 100.68batch/s, batch_nb=15, loss=0.572, v_nb=118]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 100.68batch/s, batch_nb=15, loss=0.572, v_nb=118]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 100.88batch/s, batch_nb=15, loss=0.563, v_nb=118]\n",
      "Epoch 14:  78%|███████▊  | 25/32 [00:00<00:00, 107.55batch/s, batch_nb=15, loss=0.563, v_nb=118]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 107.55batch/s, batch_nb=15, loss=0.563, v_nb=118]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 103.12batch/s, batch_nb=15, loss=0.554, v_nb=118]\n",
      "Epoch 15:  78%|███████▊  | 25/32 [00:00<00:00, 109.83batch/s, batch_nb=15, loss=0.554, v_nb=118]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 109.83batch/s, batch_nb=15, loss=0.554, v_nb=118]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 105.86batch/s, batch_nb=15, loss=0.545, v_nb=118]\n",
      "Epoch 16:  81%|████████▏ | 26/32 [00:00<00:00, 112.82batch/s, batch_nb=15, loss=0.545, v_nb=118]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 112.82batch/s, batch_nb=15, loss=0.545, v_nb=118]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 105.87batch/s, batch_nb=15, loss=0.536, v_nb=118]\n",
      "Epoch 17:  78%|███████▊  | 25/32 [00:00<00:00, 111.09batch/s, batch_nb=15, loss=0.536, v_nb=118]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 111.09batch/s, batch_nb=15, loss=0.536, v_nb=118]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 106.45batch/s, batch_nb=15, loss=0.528, v_nb=118]\n",
      "Epoch 18:  81%|████████▏ | 26/32 [00:00<00:00, 113.20batch/s, batch_nb=15, loss=0.528, v_nb=118]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 113.20batch/s, batch_nb=15, loss=0.528, v_nb=118]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 107.81batch/s, batch_nb=15, loss=0.520, v_nb=118]\n",
      "Epoch 19:  81%|████████▏ | 26/32 [00:00<00:00, 114.02batch/s, batch_nb=15, loss=0.520, v_nb=118]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 114.02batch/s, batch_nb=15, loss=0.520, v_nb=118]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 109.22batch/s, batch_nb=15, loss=0.512, v_nb=118]\n",
      "Epoch 20:  84%|████████▍ | 27/32 [00:00<00:00, 115.11batch/s, batch_nb=15, loss=0.512, v_nb=118]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 115.11batch/s, batch_nb=15, loss=0.512, v_nb=118]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 108.49batch/s, batch_nb=15, loss=0.504, v_nb=118]\n",
      "Epoch 21:  78%|███████▊  | 25/32 [00:00<00:00, 112.10batch/s, batch_nb=15, loss=0.504, v_nb=118]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 112.10batch/s, batch_nb=15, loss=0.504, v_nb=118]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 103.82batch/s, batch_nb=15, loss=0.496, v_nb=118]\n",
      "Epoch 22:  78%|███████▊  | 25/32 [00:00<00:00, 110.21batch/s, batch_nb=15, loss=0.496, v_nb=118]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 110.21batch/s, batch_nb=15, loss=0.496, v_nb=118]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 107.53batch/s, batch_nb=15, loss=0.488, v_nb=118]\n",
      "Epoch 23:  72%|███████▏  | 23/32 [00:00<00:00, 96.27batch/s, batch_nb=15, loss=0.488, v_nb=118] \n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 96.27batch/s, batch_nb=15, loss=0.488, v_nb=118]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 97.54batch/s, batch_nb=15, loss=0.479, v_nb=118]\n",
      "Epoch 24:  75%|███████▌  | 24/32 [00:00<00:00, 103.16batch/s, batch_nb=15, loss=0.479, v_nb=118]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 103.16batch/s, batch_nb=15, loss=0.479, v_nb=118]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 102.19batch/s, batch_nb=15, loss=0.472, v_nb=118]\n",
      "Epoch 25:  69%|██████▉   | 22/32 [00:00<00:00, 103.92batch/s, batch_nb=15, loss=0.472, v_nb=118]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 103.92batch/s, batch_nb=15, loss=0.472, v_nb=118]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 113.16batch/s, batch_nb=15, loss=0.472, v_nb=118]\n",
      "iteration: 0, accuracy: 0.859179675579071:  33%|███▎      | 1/3 [00:07<00:14,  7.19s/it]INFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1  cosine_sim   CosineSimilarity    0  \n",
      "2     loss_fn  BCEWithLogitsLoss    0  \n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 102.46batch/s, batch_nb=15, loss=0.744, v_nb=119]\n",
      "Epoch 1:  69%|██████▉   | 22/32 [00:00<00:00, 103.46batch/s, batch_nb=15, loss=0.744, v_nb=119]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 103.46batch/s, batch_nb=15, loss=0.744, v_nb=119]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 102.13batch/s, batch_nb=15, loss=0.736, v_nb=119]\n",
      "Epoch 2:  72%|███████▏  | 23/32 [00:00<00:00, 107.90batch/s, batch_nb=15, loss=0.736, v_nb=119]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 107.90batch/s, batch_nb=15, loss=0.736, v_nb=119]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 104.40batch/s, batch_nb=15, loss=0.730, v_nb=119]\n",
      "Epoch 3:  72%|███████▏  | 23/32 [00:00<00:00, 110.06batch/s, batch_nb=15, loss=0.730, v_nb=119]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 110.06batch/s, batch_nb=15, loss=0.730, v_nb=119]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 104.60batch/s, batch_nb=15, loss=0.725, v_nb=119]\n",
      "Epoch 4:  75%|███████▌  | 24/32 [00:00<00:00, 111.72batch/s, batch_nb=15, loss=0.725, v_nb=119]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 111.72batch/s, batch_nb=15, loss=0.725, v_nb=119]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 110.32batch/s, batch_nb=15, loss=0.720, v_nb=119]\n",
      "Epoch 5:  75%|███████▌  | 24/32 [00:00<00:00, 113.84batch/s, batch_nb=15, loss=0.720, v_nb=119]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 113.84batch/s, batch_nb=15, loss=0.720, v_nb=119]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 107.47batch/s, batch_nb=15, loss=0.715, v_nb=119]\n",
      "Epoch 6:  81%|████████▏ | 26/32 [00:00<00:00, 114.82batch/s, batch_nb=15, loss=0.715, v_nb=119]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 114.82batch/s, batch_nb=15, loss=0.715, v_nb=119]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 111.40batch/s, batch_nb=15, loss=0.706, v_nb=119]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 121.43batch/s, batch_nb=15, loss=0.706, v_nb=119]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 117.01batch/s, batch_nb=15, loss=0.697, v_nb=119]\n",
      "Epoch 8:  84%|████████▍ | 27/32 [00:00<00:00, 121.51batch/s, batch_nb=15, loss=0.697, v_nb=119]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 121.51batch/s, batch_nb=15, loss=0.697, v_nb=119]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 116.10batch/s, batch_nb=15, loss=0.689, v_nb=119]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 124.38batch/s, batch_nb=15, loss=0.689, v_nb=119]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 118.42batch/s, batch_nb=15, loss=0.681, v_nb=119]\n",
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 125.83batch/s, batch_nb=15, loss=0.681, v_nb=119]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 118.83batch/s, batch_nb=15, loss=0.675, v_nb=119]\n",
      "Epoch 11:  88%|████████▊ | 28/32 [00:00<00:00, 125.66batch/s, batch_nb=15, loss=0.675, v_nb=119]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 125.66batch/s, batch_nb=15, loss=0.675, v_nb=119]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 117.64batch/s, batch_nb=15, loss=0.669, v_nb=119]\n",
      "Epoch 12:  84%|████████▍ | 27/32 [00:00<00:00, 123.54batch/s, batch_nb=15, loss=0.669, v_nb=119]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 123.54batch/s, batch_nb=15, loss=0.669, v_nb=119]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 117.40batch/s, batch_nb=15, loss=0.664, v_nb=119]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 127.15batch/s, batch_nb=15, loss=0.664, v_nb=119]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 118.68batch/s, batch_nb=15, loss=0.660, v_nb=119]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 128.13batch/s, batch_nb=15, loss=0.660, v_nb=119]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 119.49batch/s, batch_nb=15, loss=0.654, v_nb=119]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 128.43batch/s, batch_nb=15, loss=0.654, v_nb=119]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 120.73batch/s, batch_nb=15, loss=0.651, v_nb=119]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 128.93batch/s, batch_nb=15, loss=0.651, v_nb=119]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 121.78batch/s, batch_nb=15, loss=0.647, v_nb=119]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 131.46batch/s, batch_nb=15, loss=0.647, v_nb=119]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 122.01batch/s, batch_nb=15, loss=0.644, v_nb=119]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 130.87batch/s, batch_nb=15, loss=0.644, v_nb=119]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 121.99batch/s, batch_nb=15, loss=0.641, v_nb=119]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 129.69batch/s, batch_nb=15, loss=0.641, v_nb=119]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 122.61batch/s, batch_nb=15, loss=0.638, v_nb=119]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 131.83batch/s, batch_nb=15, loss=0.638, v_nb=119]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 122.22batch/s, batch_nb=15, loss=0.636, v_nb=119]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 129.92batch/s, batch_nb=15, loss=0.636, v_nb=119]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 122.48batch/s, batch_nb=15, loss=0.634, v_nb=119]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 132.02batch/s, batch_nb=15, loss=0.634, v_nb=119]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 122.97batch/s, batch_nb=15, loss=0.631, v_nb=119]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 129.41batch/s, batch_nb=15, loss=0.631, v_nb=119]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 119.97batch/s, batch_nb=15, loss=0.630, v_nb=119]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 128.64batch/s, batch_nb=15, loss=0.630, v_nb=119]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 119.55batch/s, batch_nb=15, loss=0.628, v_nb=119]\n",
      "Epoch 25:  84%|████████▍ | 27/32 [00:00<00:00, 122.50batch/s, batch_nb=15, loss=0.628, v_nb=119]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 122.50batch/s, batch_nb=15, loss=0.628, v_nb=119]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 117.25batch/s, batch_nb=15, loss=0.628, v_nb=119]\n",
      "iteration: 1, accuracy: 0.4837890565395355:  67%|██████▋   | 2/3 [00:13<00:07,  7.01s/it]INFO:root:         Name               Type Params\n",
      "0          l1             Linear    3 K\n",
      "1  cosine_sim   CosineSimilarity    0  \n",
      "2     loss_fn  BCEWithLogitsLoss    0  \n",
      "Epoch 1:  50%|█████     | 16/32 [00:00<00:00, 109.41batch/s, batch_nb=15, loss=0.789, v_nb=120]\n",
      "Epoch 1:  75%|███████▌  | 24/32 [00:00<00:00, 114.50batch/s, batch_nb=15, loss=0.789, v_nb=120]\n",
      "Epoch 1: 100%|██████████| 32/32 [00:00<00:00, 114.50batch/s, batch_nb=15, loss=0.789, v_nb=120]\n",
      "Epoch 2:  50%|█████     | 16/32 [00:00<00:00, 112.20batch/s, batch_nb=15, loss=0.782, v_nb=120]\n",
      "Epoch 2:  72%|███████▏  | 23/32 [00:00<00:00, 114.26batch/s, batch_nb=15, loss=0.782, v_nb=120]\n",
      "Epoch 2: 100%|██████████| 32/32 [00:00<00:00, 114.26batch/s, batch_nb=15, loss=0.782, v_nb=120]\n",
      "Epoch 3:  50%|█████     | 16/32 [00:00<00:00, 112.48batch/s, batch_nb=15, loss=0.777, v_nb=120]\n",
      "Epoch 3: 100%|██████████| 32/32 [00:00<00:00, 118.46batch/s, batch_nb=15, loss=0.777, v_nb=120]\n",
      "Epoch 4:  50%|█████     | 16/32 [00:00<00:00, 115.00batch/s, batch_nb=15, loss=0.773, v_nb=120]\n",
      "Epoch 4: 100%|██████████| 32/32 [00:00<00:00, 122.27batch/s, batch_nb=15, loss=0.773, v_nb=120]\n",
      "Epoch 5:  50%|█████     | 16/32 [00:00<00:00, 115.93batch/s, batch_nb=15, loss=0.770, v_nb=120]\n",
      "Epoch 5:  88%|████████▊ | 28/32 [00:00<00:00, 124.12batch/s, batch_nb=15, loss=0.770, v_nb=120]\n",
      "Epoch 5: 100%|██████████| 32/32 [00:00<00:00, 124.12batch/s, batch_nb=15, loss=0.770, v_nb=120]\n",
      "Epoch 6:  50%|█████     | 16/32 [00:00<00:00, 116.10batch/s, batch_nb=15, loss=0.767, v_nb=120]\n",
      "Epoch 6:  88%|████████▊ | 28/32 [00:00<00:00, 123.31batch/s, batch_nb=15, loss=0.767, v_nb=120]\n",
      "Epoch 6: 100%|██████████| 32/32 [00:00<00:00, 123.31batch/s, batch_nb=15, loss=0.767, v_nb=120]\n",
      "Epoch 7:  50%|█████     | 16/32 [00:00<00:00, 113.94batch/s, batch_nb=15, loss=0.762, v_nb=120]\n",
      "Epoch 7: 100%|██████████| 32/32 [00:00<00:00, 122.59batch/s, batch_nb=15, loss=0.762, v_nb=120]\n",
      "Epoch 8:  50%|█████     | 16/32 [00:00<00:00, 112.58batch/s, batch_nb=15, loss=0.756, v_nb=120]\n",
      "Epoch 8: 100%|██████████| 32/32 [00:00<00:00, 122.80batch/s, batch_nb=15, loss=0.756, v_nb=120]\n",
      "Epoch 9:  50%|█████     | 16/32 [00:00<00:00, 110.58batch/s, batch_nb=15, loss=0.751, v_nb=120]\n",
      "Epoch 9:  88%|████████▊ | 28/32 [00:00<00:00, 118.01batch/s, batch_nb=15, loss=0.751, v_nb=120]\n",
      "Epoch 9: 100%|██████████| 32/32 [00:00<00:00, 118.01batch/s, batch_nb=15, loss=0.751, v_nb=120]\n",
      "Epoch 10:  50%|█████     | 16/32 [00:00<00:00, 112.18batch/s, batch_nb=15, loss=0.747, v_nb=120]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 32/32 [00:00<00:00, 120.18batch/s, batch_nb=15, loss=0.747, v_nb=120]\n",
      "Epoch 11:  50%|█████     | 16/32 [00:00<00:00, 116.39batch/s, batch_nb=15, loss=0.743, v_nb=120]\n",
      "Epoch 11: 100%|██████████| 32/32 [00:00<00:00, 123.78batch/s, batch_nb=15, loss=0.743, v_nb=120]\n",
      "Epoch 12:  50%|█████     | 16/32 [00:00<00:00, 119.91batch/s, batch_nb=15, loss=0.739, v_nb=120]\n",
      "Epoch 12: 100%|██████████| 32/32 [00:00<00:00, 127.80batch/s, batch_nb=15, loss=0.739, v_nb=120]\n",
      "Epoch 13:  50%|█████     | 16/32 [00:00<00:00, 116.00batch/s, batch_nb=15, loss=0.736, v_nb=120]\n",
      "Epoch 13: 100%|██████████| 32/32 [00:00<00:00, 124.77batch/s, batch_nb=15, loss=0.736, v_nb=120]\n",
      "Epoch 14:  50%|█████     | 16/32 [00:00<00:00, 116.16batch/s, batch_nb=15, loss=0.733, v_nb=120]\n",
      "Epoch 14: 100%|██████████| 32/32 [00:00<00:00, 125.04batch/s, batch_nb=15, loss=0.733, v_nb=120]\n",
      "Epoch 15:  50%|█████     | 16/32 [00:00<00:00, 116.53batch/s, batch_nb=15, loss=0.731, v_nb=120]\n",
      "Epoch 15: 100%|██████████| 32/32 [00:00<00:00, 125.86batch/s, batch_nb=15, loss=0.731, v_nb=120]\n",
      "Epoch 16:  50%|█████     | 16/32 [00:00<00:00, 120.81batch/s, batch_nb=15, loss=0.727, v_nb=120]\n",
      "Epoch 16: 100%|██████████| 32/32 [00:00<00:00, 128.16batch/s, batch_nb=15, loss=0.727, v_nb=120]\n",
      "Epoch 17:  50%|█████     | 16/32 [00:00<00:00, 117.01batch/s, batch_nb=15, loss=0.726, v_nb=120]\n",
      "Epoch 17: 100%|██████████| 32/32 [00:00<00:00, 125.07batch/s, batch_nb=15, loss=0.726, v_nb=120]\n",
      "Epoch 18:  50%|█████     | 16/32 [00:00<00:00, 118.59batch/s, batch_nb=15, loss=0.723, v_nb=120]\n",
      "Epoch 18:  91%|█████████ | 29/32 [00:00<00:00, 125.33batch/s, batch_nb=15, loss=0.723, v_nb=120]\n",
      "Epoch 18: 100%|██████████| 32/32 [00:00<00:00, 125.33batch/s, batch_nb=15, loss=0.723, v_nb=120]\n",
      "Epoch 19:  50%|█████     | 16/32 [00:00<00:00, 120.65batch/s, batch_nb=15, loss=0.722, v_nb=120]\n",
      "Epoch 19: 100%|██████████| 32/32 [00:00<00:00, 127.90batch/s, batch_nb=15, loss=0.722, v_nb=120]\n",
      "Epoch 20:  50%|█████     | 16/32 [00:00<00:00, 117.35batch/s, batch_nb=15, loss=0.718, v_nb=120]\n",
      "Epoch 20: 100%|██████████| 32/32 [00:00<00:00, 125.95batch/s, batch_nb=15, loss=0.718, v_nb=120]\n",
      "Epoch 21:  50%|█████     | 16/32 [00:00<00:00, 118.05batch/s, batch_nb=15, loss=0.718, v_nb=120]\n",
      "Epoch 21:  88%|████████▊ | 28/32 [00:00<00:00, 123.20batch/s, batch_nb=15, loss=0.718, v_nb=120]\n",
      "Epoch 21: 100%|██████████| 32/32 [00:00<00:00, 123.20batch/s, batch_nb=15, loss=0.718, v_nb=120]\n",
      "Epoch 22:  50%|█████     | 16/32 [00:00<00:00, 117.12batch/s, batch_nb=15, loss=0.715, v_nb=120]\n",
      "Epoch 22:  84%|████████▍ | 27/32 [00:00<00:00, 121.14batch/s, batch_nb=15, loss=0.715, v_nb=120]\n",
      "Epoch 22: 100%|██████████| 32/32 [00:00<00:00, 121.14batch/s, batch_nb=15, loss=0.715, v_nb=120]\n",
      "Epoch 23:  50%|█████     | 16/32 [00:00<00:00, 115.87batch/s, batch_nb=15, loss=0.714, v_nb=120]\n",
      "Epoch 23: 100%|██████████| 32/32 [00:00<00:00, 123.98batch/s, batch_nb=15, loss=0.714, v_nb=120]\n",
      "Epoch 24:  50%|█████     | 16/32 [00:00<00:00, 118.56batch/s, batch_nb=15, loss=0.713, v_nb=120]\n",
      "Epoch 24: 100%|██████████| 32/32 [00:00<00:00, 126.96batch/s, batch_nb=15, loss=0.713, v_nb=120]\n",
      "Epoch 25:  50%|█████     | 16/32 [00:00<00:00, 120.55batch/s, batch_nb=15, loss=0.712, v_nb=120]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 130.95batch/s, batch_nb=15, loss=0.712, v_nb=120]\n",
      "Epoch 25: 100%|██████████| 32/32 [00:00<00:00, 131.79batch/s, batch_nb=15, loss=0.712, v_nb=120]\n",
      "iteration: 2, accuracy: 0.510937511920929: 100%|██████████| 3/3 [00:20<00:00,  6.77s/it] \n"
     ]
    }
   ],
   "source": [
    "#inlp_model_handler.train_model(inlp_dataset)\n",
    "P, rowspace_projections, Ws = inlp.run_INLP(num_classifiers = 3, input_dim = 100, is_autoregressive = True, min_accuracy = 0, dataset_handler = inlp_dataset, model = inlp_model_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note that the cosine/l2 distance loss is no longer convex, so w_i.dot(w_j) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02186606,  0.0346785 , -0.02964997, ...,  0.02251494,\n",
       "         0.06070557,  0.03205736],\n",
       "       [-0.00448008, -0.02645985,  0.03417505, ..., -0.00667388,\n",
       "        -0.01322363, -0.00543375],\n",
       "       [-0.00049647,  0.01373247, -0.02147561, ...,  0.00711308,\n",
       "         0.08852425,  0.019908  ],\n",
       "       ...,\n",
       "       [ 0.01660385,  0.02621097, -0.01645809, ..., -0.02676257,\n",
       "        -0.05707281, -0.02487635],\n",
       "       [-0.00046501,  0.00114721,  0.09579111, ..., -0.02665293,\n",
       "        -0.15518732, -0.00907945],\n",
       "       [-0.00971164,  0.05872897, -0.03290517, ...,  0.00484753,\n",
       "         0.0505405 ,  0.00218303]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[0].dot(Ws[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69745433042437e-06\n",
      "0.000550595020200669\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(P.dot(P) - P))\n",
    "print( np.linalg.norm( Ws[-1][:32, :].dot(P.dot(x_train1[0]))) ) # note that the norm is not exactly 0 due to pytorch floating point precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
