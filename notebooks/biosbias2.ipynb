{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../data/embeddings\")\n",
    "sys.path.append(\"../data/embeddings/biasbios\")\n",
    "import classifier\n",
    "import svm_classifier\n",
    "import debias\n",
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname = \"../data/data.biosbias.pickle\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def count_profs_and_gender(data: List[dict]):\n",
    "    \n",
    "    counter = defaultdict(Counter)\n",
    "    for entry in data:\n",
    "        gender, prof = entry[\"gender\"], entry[\"raw_title\"]\n",
    "        counter[prof.lower()][gender.lower()] += 1\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def filter_dataset(data, topk = 10):\n",
    "    \n",
    "    filtered = []\n",
    "    counter = count_profs_and_gender(data)\n",
    "    total_counts = [(prof, counter[prof][\"f\"] + counter[prof][\"m\"]) for prof in counter.keys()]\n",
    "    profs_by_frq = sorted(total_counts, key = lambda x: -x[1])\n",
    "    topk_profs = [p[0] for p in profs_by_frq[:topk]]\n",
    "    \n",
    "    print(\"Top-k professions: {}\".format(topk_profs))\n",
    "    for d in data:\n",
    "        \n",
    "        if d[\"raw_title\"].lower() in topk_profs:\n",
    "            filtered.append(d)\n",
    "    \n",
    "    return filtered\n",
    "    \n",
    "def split_train_dev_test(data):\n",
    "    \n",
    "    g2i, i2g = {\"m\": 0, \"f\": 1}, {1: \"f\", 0: \"m\"}\n",
    "    all_profs = list(set([d[\"raw_title\"].lower() for d in data]))\n",
    "    all_words = []\n",
    "    for d in data:\n",
    "        all_words.extend(d[\"raw\"].split(\" \"))\n",
    "    \n",
    "    all_words = set(all_words)\n",
    "    all_words.add(\"<UNK>\")\n",
    "    \n",
    "    p2i = {p:i for i,p in enumerate(sorted(all_profs))}\n",
    "    i2p = {i:p for i,p in enumerate(sorted(all_profs))}\n",
    "    w2i = {w:i for i,w in enumerate(sorted(all_words))}\n",
    "    i2w = {i:w for i,w in enumerate(sorted(all_words))}\n",
    "    \n",
    "    all_data = []\n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        gender, prof = entry[\"gender\"].lower(), entry[\"raw_title\"].lower()\n",
    "        raw, start_index = entry[\"raw\"], entry[\"start_pos\"]\n",
    "        all_data.append({\"g\": g2i[gender], \"p\": p2i[prof], \"text\": raw, \"start\": start_index})\n",
    "\n",
    "\n",
    "    train_dev, test = sklearn.model_selection.train_test_split(all_data, test_size = 0.2, random_state = 0)\n",
    "    train, dev = sklearn.model_selection.train_test_split(train_dev, test_size = 0.3, random_state = 0)\n",
    "    print(\"Train size: {}; Dev size: {}; Test size: {}\".format(len(train), len(dev), len(test)))\n",
    "    return (train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k professions: ['associate professor', 'assistant professor', 'attorney', 'journalist', 'photographer', 'teacher', 'psychologist', 'physician', 'architect', 'poet', 'nurse', 'painter', 'filmmaker', 'composer', 'model', 'software engineer', 'dentist', 'surgeon', 'comedian', 'psychotherapist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141473/141473 [00:00<00:00, 699505.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 79224; Dev size: 33954; Test size: 28295\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "counter = count_profs_and_gender(data)\n",
    "data = filter_dataset(data, topk = 20)\n",
    "(train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w) = split_train_dev_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'assistant professor': Counter({'f': 12301, 'm': 14996}),\n",
       "             'certified public accountant': Counter({'m': 280, 'f': 159}),\n",
       "             'journalist': Counter({'f': 5936, 'm': 6238}),\n",
       "             'architect': Counter({'m': 3565, 'f': 918}),\n",
       "             'photographer': Counter({'m': 7753, 'f': 3688}),\n",
       "             'psychologist': Counter({'m': 2325, 'f': 3193}),\n",
       "             'teacher': Counter({'f': 3487, 'm': 2770}),\n",
       "             'nurse': Counter({'f': 3117, 'm': 285}),\n",
       "             'associate professor': Counter({'f': 11640, 'm': 17143}),\n",
       "             'attorney': Counter({'f': 4677, 'm': 8197}),\n",
       "             'software engineer': Counter({'m': 2054, 'f': 375}),\n",
       "             'painter': Counter({'f': 1369, 'm': 1755}),\n",
       "             'physician': Counter({'m': 2768, 'f': 1873}),\n",
       "             'chiropractor': Counter({'m': 481, 'f': 201}),\n",
       "             'personal trainer': Counter({'m': 151, 'f': 113}),\n",
       "             'licensed acupuncturist': Counter({'f': 117, 'm': 78}),\n",
       "             'surgeon': Counter({'m': 1561, 'f': 313}),\n",
       "             'filmmaker': Counter({'f': 996, 'm': 2122}),\n",
       "             'dietitian': Counter({'f': 1199, 'm': 62}),\n",
       "             'dentist': Counter({'m': 1383, 'f': 673}),\n",
       "             'dj': Counter({'m': 532, 'f': 122}),\n",
       "             'model': Counter({'f': 2222, 'm': 517}),\n",
       "             'composer': Counter({'m': 2494, 'f': 485}),\n",
       "             'trial attorney': Counter({'f': 150, 'm': 532}),\n",
       "             'psychotherapist': Counter({'m': 394, 'f': 869}),\n",
       "             'poet': Counter({'f': 1816, 'm': 1720}),\n",
       "             'trial lawyer': Counter({'m': 743, 'f': 126}),\n",
       "             'plastic surgeon': Counter({'m': 811, 'f': 186}),\n",
       "             'comedian': Counter({'m': 1219, 'f': 266}),\n",
       "             'yoga instructor': Counter({'f': 186, 'm': 31}),\n",
       "             'interior designer': Counter({'f': 467, 'm': 120}),\n",
       "             'senior software engineer': Counter({'f': 63, 'm': 587}),\n",
       "             'pastor': Counter({'m': 756, 'f': 317}),\n",
       "             'landscape architect': Counter({'m': 188, 'f': 137}),\n",
       "             'rapper': Counter({'m': 537, 'f': 43}),\n",
       "             'yoga teacher': Counter({'f': 304, 'm': 72}),\n",
       "             'orthopedic surgeon': Counter({'m': 762, 'f': 52}),\n",
       "             'accountant': Counter({'m': 639, 'f': 326}),\n",
       "             'neurosurgeon': Counter({'m': 431, 'f': 37}),\n",
       "             'cpa': Counter({'m': 230, 'f': 167}),\n",
       "             'certified personal trainer': Counter({'m': 97, 'f': 121}),\n",
       "             'nutritionist': Counter({'m': 58, 'f': 267}),\n",
       "             'licensed real': Counter({'m': 198, 'f': 145}),\n",
       "             'magician': Counter({'m': 330, 'f': 38}),\n",
       "             'paralegal': Counter({'f': 377, 'm': 66}),\n",
       "             'licensed massage therapist': Counter({'m': 60, 'f': 156})})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input representatons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(fname = \"../data/embeddings/vecs.filtered.with_gendered.glove.txt\"):\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\n",
    "def get_embeddings_based_dataset(data: List[dict], word2vec_model):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        bagofwords = np.sum([word2vec_model[w] for w in words if w in word2vec_model], axis = 0)\n",
    "        X.append(bagofwords)\n",
    "        Y.append(y)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "def get_BOW_based_dataset(data: List[dict], w2i):\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = True)\n",
    "    X, Y = [], []\n",
    "    data_dicts = []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        entry_dict = {w:w2i[w] if w in w2i else w2i[\"<UNK>\"] for w in words}\n",
    "        data_dicts.append(entry_dict)\n",
    "        Y.append(y)\n",
    "    \n",
    "    X = vectorizer.fit_transform(data_dicts)\n",
    "    return X,Y\n",
    "    \n",
    "def get_bert_based_dataset(data: List[dict]):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()            \n",
    "        tokenized_text = tokenizer.tokenize(sentence_str)    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "              outputs = model(tokens_tensor) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79224/79224 [00:10<00:00, 7403.71it/s]\n",
      "100%|██████████| 33954/33954 [00:04<00:00, 7266.17it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\") #load_word2vec()\n",
    "#X_train, Y_train = get_BOW_based_dataset(train, w2i)\n",
    "#X_dev, Y_dev = get_BOW_based_dataset(dev, w2i) \n",
    "X_train, Y_train = get_embeddings_based_dataset(train, word2vec)\n",
    "X_dev, Y_dev =  get_embeddings_based_dataset(dev, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   57.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 28 seconds\n",
      "max_iter reached after 29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 30 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:  1.9min remaining:   28.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "max_iter reached after 29 seconds\n",
      "0.5984861871944396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "#clf = LinearSVC(verbose = 10) #LogisticRegression()\n",
    "clf = LogisticRegression(solver = \"sag\", verbose = 10, n_jobs = 4)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
