{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../data/embeddings\")\n",
    "sys.path.append(\"../data/embeddings/biasbios\")\n",
    "import classifier\n",
    "import svm_classifier\n",
    "import debias\n",
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import copy\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "PROFS = ['professor', 'physician', 'attorney', 'photographer', 'journalist', 'nurse', 'psychologist', 'teacher',\n",
    "'dentist', 'surgeon', 'architect', 'painter', 'model', 'poet', 'filmmaker', 'software_engineer',\n",
    "'accountant', 'composer', 'dietitian', 'comedian', 'chiropractor', 'pastor', 'paralegal', 'yoga_teacher',\n",
    "'dj', 'interior_designer', 'personal_trainer', 'rapper']\n",
    "\n",
    "PROF2UNIFIED_PROF = {\"associate professor\": \"professor\", \"assistant professor\": \"professor\", \"software engineer\": \"software_engineer\", \"psychotherapist\": \"psychologist\", \"orthopedic surgeon\": \"surgeon\", \"trial lawyer\": \"attorney\",\"plastic surgeon\": \"surgeon\",  \"trial attorney\": \"attorney\", \"senior software engineer\": \"software_engineer\", \"interior designer\": \"interior_designer\", \"certified public accountant\": \"accountant\", \"cpa\": \"accountant\", \"neurosurgeon\": \"surgeon\", \"yoga teacher\": \"yoga_teacher\", \"nutritionist\": \"dietitian\", \"personal trainer\": \"personal_trainer\", \"certified personal trainer\": \"personal_trainer\", \"yoga instructor\": \"yoga_teacher\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname = \"../data/BIOS.pkl\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    for i, data_dict in enumerate(data):\n",
    "        \n",
    "        prof = data_dict[\"raw_title\"].lower()\n",
    "        data[i][\"raw_title\"] = PROF2UNIFIED_PROF[prof] if prof in PROF2UNIFIED_PROF else prof\n",
    "    \n",
    "    return data\n",
    "        \n",
    "        \n",
    "    \n",
    "def count_profs_and_gender(data: List[dict]):\n",
    "    \n",
    "    counter = defaultdict(Counter)\n",
    "    for entry in data:\n",
    "        gender, prof = entry[\"gender\"], entry[\"raw_title\"]\n",
    "        counter[prof.lower()][gender.lower()] += 1\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def filter_dataset(data, topk = 10):\n",
    "    \n",
    "    filtered = []\n",
    "    counter = count_profs_and_gender(data)\n",
    "    total_counts = [(prof, counter[prof][\"f\"] + counter[prof][\"m\"]) for prof in counter.keys()]\n",
    "    profs_by_frq = sorted(total_counts, key = lambda x: -x[1])\n",
    "    topk_profs = [p[0] for p in profs_by_frq[:topk]]\n",
    "    \n",
    "    print(\"Top-k professions: {}\".format(topk_profs))\n",
    "    for d in data:\n",
    "        \n",
    "        if d[\"raw_title\"].lower() in topk_profs:\n",
    "            filtered.append(d)\n",
    "    \n",
    "    return filtered\n",
    "    \n",
    "def split_train_dev_test(data):\n",
    "    \n",
    "    g2i, i2g = {\"m\": 0, \"f\": 1}, {1: \"f\", 0: \"m\"}\n",
    "    all_profs = list(set([d[\"raw_title\"] for d in data]))\n",
    "    all_words = []\n",
    "    for d in data:\n",
    "        all_words.extend(d[\"raw\"].split(\" \"))\n",
    "    \n",
    "    all_words = set(all_words)\n",
    "    all_words.add(\"<UNK>\")\n",
    "    \n",
    "    p2i = {p:i for i,p in enumerate(sorted(all_profs))}\n",
    "    i2p = {i:p for i,p in enumerate(sorted(all_profs))}\n",
    "    w2i = {w:i for i,w in enumerate(sorted(all_words))}\n",
    "    i2w = {i:w for i,w in enumerate(sorted(all_words))}\n",
    "    \n",
    "    all_data = []\n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        gender, prof = entry[\"gender\"].lower(), entry[\"raw_title\"].lower()\n",
    "        #if prof in PROF2UNITED_PROF: prof = PROF2UNITED_PROF[prof]\n",
    "        raw, start_index = entry[\"raw\"], entry[\"start_pos\"]\n",
    "        all_data.append({\"g\": g2i[gender], \"p\": p2i[prof], \"text\": raw, \"start\": start_index})\n",
    "\n",
    "\n",
    "    train_dev, test = sklearn.model_selection.train_test_split(all_data, test_size = 0.2, random_state = 0)\n",
    "    train, dev = sklearn.model_selection.train_test_split(train_dev, test_size = 0.3, random_state = 0)\n",
    "    print(\"Train size: {}; Dev size: {}; Test size: {}\".format(len(train), len(dev), len(test)))\n",
    "    return (train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k professions: ['professor', 'physician', 'attorney', 'photographer', 'journalist', 'nurse', 'psychologist', 'teacher', 'architect', 'surgeon', 'dentist', 'painter', 'poet', 'model', 'filmmaker', 'software_engineer', 'composer', 'accountant', 'dietitian', 'comedian', 'pastor', 'chiropractor', 'yoga_teacher', 'paralegal', 'interior_designer', 'dj', 'rapper', 'personal_trainer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236263/236263 [00:00<00:00, 862970.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 132307; Dev size: 56703; Test size: 47253\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "data = filter_dataset(data, topk = 60)\n",
    "(train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w) = split_train_dev_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4644654474039524\n"
     ]
    }
   ],
   "source": [
    "f,m = 0., 0.\n",
    "for k, values in counter.items():\n",
    "    f += values['f']\n",
    "    m += values['m']\n",
    "\n",
    "print(f / (f + m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accountant': 0, 'architect': 1, 'attorney': 2, 'chiropractor': 3, 'comedian': 4, 'composer': 5, 'dentist': 6, 'dietitian': 7, 'dj': 8, 'filmmaker': 9, 'interior_designer': 10, 'journalist': 11, 'model': 12, 'nurse': 13, 'painter': 14, 'paralegal': 15, 'pastor': 16, 'personal_trainer': 17, 'photographer': 18, 'physician': 19, 'poet': 20, 'professor': 21, 'psychologist': 22, 'rapper': 23, 'software_engineer': 24, 'surgeon': 25, 'teacher': 26, 'yoga_teacher': 27}\n"
     ]
    }
   ],
   "source": [
    "print(p2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input representatons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(fname = \"../data/embeddings/vecs.filtered.with_gendered.glove.txt\"):\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\n",
    "def get_embeddings_based_dataset(data: List[dict], word2vec_model):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        #print(text)\n",
    "        #print(\"----------\")\n",
    "        #print(\" \".join(words))\n",
    "        #print(\"=====================\")\n",
    "        bagofwords = np.sum([word2vec_model[w] if w in word2vec_model else word2vec_model['unk'] for w in words], axis = 0)\n",
    "        X.append(bagofwords)\n",
    "        Y.append(y)\n",
    "        total += len(words)\n",
    "        unk += len([w for w in words if w not in word2vec_model])\n",
    "    \n",
    "    print(\"% unknown: {}\".format(unk/total))\n",
    "    return X,Y\n",
    "\n",
    "def get_BOW_based_dataset(data: List[dict], w2i):\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = True)\n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    data_dicts = []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        entry_dict = {w:w2i[w] if w in w2i else w2i[\"<UNK>\"] for w in words}\n",
    "        data_dicts.append(entry_dict)\n",
    "        Y.append(y)\n",
    "        \n",
    "        total += len(words)\n",
    "        unk += len([w for w in words if w not in w2i])\n",
    "    \n",
    "    print(\"% unknown: {}\".format(unk/total))\n",
    "    X = vectorizer.fit_transform(data_dicts)\n",
    "    return X,Y\n",
    "    \n",
    "def get_bert_based_dataset(data: List[dict]):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()            \n",
    "        tokenized_text = tokenizer.tokenize(sentence_str)    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "              outputs = model(tokens_tensor) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\") #load_word2vec()\n",
    "word2vec, vecs, words = load_word_vectors(\"../data/embeddings/GoogleNews-vectors-negative300-SLIM.bin\")\n",
    "#word2vec.init_sims(replace = True)\n",
    "#X_train, Y_train = get_BOW_based_dataset(train, w2i)\n",
    "#X_dev, Y_dev = get_BOW_based_dataset(dev, w2i) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132307/132307 [00:22<00:00, 5886.62it/s]\n",
      "  1%|          | 577/56703 [00:00<00:09, 5765.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% unknown: 0.28309907019755093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56703/56703 [00:09<00:00, 5862.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% unknown: 0.2829943349768876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_embeddings_based_dataset(train, word2vec)\n",
    "X_dev, Y_dev =  get_embeddings_based_dataset(dev, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clf = LinearSVC(verbose = 10) #LogisticRegression()\n",
    "clf = LogisticRegression(warm_start = True, solver = \"sag\", multi_class = 'multinomial', verbose = 0, n_jobs = 32)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf.predict(X_dev)\n",
    "cm = sklearn.metrics.confusion_matrix(Y_dev,y_hat)\n",
    "labels = [i2p[i] for i in range(len(i2p))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=0.3)#for label size\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.savefig(\"confusion.png\", dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_matrix(num_clfs, X_train, Y_train, X_dev, Y_dev, dim=300):\n",
    "\n",
    "    is_autoregressive = True\n",
    "    siamese = False\n",
    "    reg = \"l2\"\n",
    "    min_acc = 0.\n",
    "    noise = False\n",
    "    random_subset = False\n",
    "    regression = False\n",
    "\n",
    "    P = debias.get_debiasing_projection(None, num_clfs, dim, is_autoregressive, min_acc, X_train, Y_train, X_dev, Y_dev,\n",
    "                                              noise = noise, random_subset = random_subset,\n",
    "                                              regression = regression, siamese = siamese, siamese_dim = 1)\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "num_clfs = 30\n",
    "Y_dev_gender = np.array([d[\"g\"] for d in dev])\n",
    "Y_train_gender = np.array([d[\"g\"] for d in train])\n",
    "P = get_projection_matrix(num_clfs, X_train, Y_train_gender, X_dev, Y_dev_gender, dim = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test model without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_dev.dot(P), Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train.dot(P), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_dev.dot(P), Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
