{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../data/embeddings\")\n",
    "sys.path.append(\"../data/biasbios\")\n",
    "sys.path.append(\"../data/embeddings/biasbios\")\n",
    "import classifier\n",
    "import svm_classifier\n",
    "import debias\n",
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import copy\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "\n",
    "\n",
    "PROFS = ['professor', 'physician', 'attorney', 'photographer', 'journalist', 'nurse', 'psychologist', 'teacher',\n",
    "'dentist', 'surgeon', 'architect', 'painter', 'model', 'poet', 'filmmaker', 'software_engineer',\n",
    "'accountant', 'composer', 'dietitian', 'comedian', 'chiropractor', 'pastor', 'paralegal', 'yoga_teacher',\n",
    "'dj', 'interior_designer', 'personal_trainer', 'rapper']\n",
    "\n",
    "PROF2UNIFIED_PROF = {\"associate professor\": \"professor\", \"assistant professor\": \"professor\", \"software engineer\": \"software_engineer\", \"psychotherapist\": \"psychologist\", \"orthopedic surgeon\": \"surgeon\", \"trial lawyer\": \"attorney\",\"plastic surgeon\": \"surgeon\",  \"trial attorney\": \"attorney\", \"senior software engineer\": \"software_engineer\", \"interior designer\": \"interior_designer\", \"certified public accountant\": \"accountant\", \"cpa\": \"accountant\", \"neurosurgeon\": \"surgeon\", \"yoga teacher\": \"yoga_teacher\", \"nutritionist\": \"dietitian\", \"personal trainer\": \"personal_trainer\", \"certified personal trainer\": \"personal_trainer\", \"yoga instructor\": \"yoga_teacher\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        \n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def load_dictionary(path):\n",
    "    \n",
    "    with open(path, \"r\", encoding = \"utf-8\") as f:\n",
    "        \n",
    "        lines = f.readlines()\n",
    "        \n",
    "    k2v, v2k = {}, {}\n",
    "    for line in lines:\n",
    "        \n",
    "        k,v = line.strip().split(\"\\t\")\n",
    "        k2v[k] = v\n",
    "        v2k[v] = k\n",
    "    \n",
    "    return k2v, v2k\n",
    "    \n",
    "def count_profs_and_gender(data: List[dict]):\n",
    "    \n",
    "    counter = defaultdict(Counter)\n",
    "    for entry in data:\n",
    "        gender, prof = entry[\"g\"], entry[\"p\"]\n",
    "        counter[prof][gender] += 1\n",
    "        \n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"../data/biasbios/train.pickle\")\n",
    "dev = load_dataset(\"../data/biasbios/dev.pickle\")\n",
    "test = load_dataset(\"../data/biasbios/test.pickle\")\n",
    "\n",
    "#w2i, i2w = load_dictionary(\"../data/biasbios/word2index.txt\")\n",
    "p2i, i2p = load_dictionary(\"../data/biasbios/profession2index.txt\")\n",
    "g2i, i2g = load_dictionary(\"../data/biasbios/gender2index.txt\")\n",
    "counter = count_profs_and_gender(train+dev+test)\n",
    "\"\"\"\n",
    "data = load_data()\n",
    "data = filter_dataset(data, topk = 60)\n",
    "counter = count_profs_and_gender(data)\n",
    "(train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w) = split_train_dev_test(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = 0., 0.\n",
    "prof2fem = dict()\n",
    "\n",
    "for k, values in counter.items():\n",
    "    f += values['f']\n",
    "    m += values['m']\n",
    "    prof2fem[k] = values['f']/(values['f'] + values['m'])\n",
    "\n",
    "print(f / (f + m))\n",
    "print(prof2fem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(p2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input representatons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def load_word_vectors(fname = \"../data/embeddings/vecs.filtered.with_gendered.glove.txt\"):\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\"\"\"\n",
    "\n",
    "def get_embeddings_based_dataset(data: List[dict], word2vec_model, p2i):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        \n",
    "        y = p2i[entry[\"p\"]]\n",
    "        words = entry[\"hard_text\"].split(\" \")\n",
    "\n",
    "        bagofwords = np.sum([word2vec_model[w] for w in words], axis = 0)\n",
    "        X.append(bagofwords)\n",
    "        Y.append(y)\n",
    "        total += len(words)\n",
    "        #unk += len([w for w in words if w not in word2vec_model])\n",
    "    \n",
    "    #print(\"% unknown: {}\".format(unk/total))\n",
    "    return X,Y\n",
    "\n",
    "def get_BOW_based_dataset(data: List[dict], w2i):\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = True)\n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    data_dicts = []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        entry_dict = {w:w2i[w] if w in w2i else w2i[\"<UNK>\"] for w in words}\n",
    "        data_dicts.append(entry_dict)\n",
    "        Y.append(y)\n",
    "        \n",
    "        total += len(words)\n",
    "        unk += len([w for w in words if w not in w2i])\n",
    "    \n",
    "    print(\"% unknown: {}\".format(unk/total))\n",
    "    X = vectorizer.fit_transform(data_dicts)\n",
    "    return X,Y\n",
    "    \n",
    "def get_bert_based_dataset(data: List[dict]):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = BertModel.from_pretrained('bert-base-cased').cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    X, Y = [], []\n",
    "    print(\"here\")\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()            \n",
    "        tokenized_text = tokenizer.tokenize(sentence_str)    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "              outputs = model(tokens_tensor)\n",
    "              print(outputs.shape)\n",
    "              return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\") #load_word2vec()\n",
    "#word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\")\n",
    "path = \"../data/embeddings/crawl-300d-2M-subword.bin\"\n",
    "word2vec = gensim.models.fasttext.load_facebook_vectors(path)\n",
    "#word2vec.init_sims(replace = True)\n",
    "#X_train, Y_train = get_BOW_based_dataset(train, w2i)\n",
    "#X_dev, Y_dev = get_BOW_based_dataset(dev, w2i) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_embeddings_based_dataset(train, word2vec, p2i)\n",
    "X_dev, Y_dev =  get_embeddings_based_dataset(dev, word2vec, p2i)\n",
    "\n",
    "#X_train, Y_train = get_bert_based_dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#clf = LinearSVC(verbose = 10) #LogisticRegression()\n",
    "clf = LogisticRegression(warm_start = True,\n",
    "                         solver = \"sag\", multi_class = 'multinomial',\n",
    "                         verbose = 0, n_jobs = 32)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_dev, Y_dev))\n",
    "clf_original = copy.deepcopy(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf.predict(X_dev)\n",
    "cm = sklearn.metrics.confusion_matrix(Y_dev,y_hat)\n",
    "labels = [i2p[i] for i in range(len(i2p))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=0.3)#for label size\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.savefig(\"confusion.png\", dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_matrix(num_clfs, X_train, Y_train, X_dev, Y_dev, dim=300):\n",
    "\n",
    "    is_autoregressive = True\n",
    "    siamese = False\n",
    "    reg = \"l2\"\n",
    "    min_acc = 0.\n",
    "    noise = False\n",
    "    random_subset = False\n",
    "    regression = False\n",
    "\n",
    "    P = debias.get_debiasing_projection(None, num_clfs, dim, is_autoregressive, min_acc, X_train, Y_train, X_dev, Y_dev,\n",
    "                                              noise = noise, random_subset = random_subset,\n",
    "                                              regression = regression, siamese = siamese, siamese_dim = 1)\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "num_clfs = 35\n",
    "Y_dev_gender = np.array([d[\"g\"] for d in dev])\n",
    "Y_train_gender = np.array([d[\"g\"] for d in train])\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)\n",
    "P = get_projection_matrix(num_clfs, X_train, Y_train_gender, X_dev, Y_dev_gender, dim = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test model without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_dev.dot(P), Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train.dot(P), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_dev.dot(P), Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TPR(y_pred, y_true, p2i, i2p, gender):\n",
    "    \n",
    "    scores = defaultdict(Counter)\n",
    "    prof_count_total = defaultdict(Counter)\n",
    "    \n",
    "    for y_hat, y, g in zip(y_pred, y_true, gender):\n",
    "        \n",
    "        if y == y_hat:\n",
    "            \n",
    "            scores[i2p[y]][g] += 1\n",
    "        \n",
    "        prof_count_total[i2p[y]][g] += 1\n",
    "    \n",
    "    tprs = defaultdict(dict)\n",
    "    tprs_change = dict()\n",
    "    tprs_ratio = []\n",
    "    \n",
    "    for profession, scores_dict in scores.items():\n",
    "        \n",
    "        good_m, good_f = scores_dict[\"m\"], scores_dict[\"f\"]\n",
    "        prof_total_f = prof_count_total[profession][\"f\"]\n",
    "        prof_total_m = prof_count_total[profession][\"m\"]\n",
    "        tpr_m = good_m / prof_total_m\n",
    "        tpr_f = good_f / prof_total_f\n",
    "        \n",
    "        tprs[profession][\"m\"] = tpr_m\n",
    "        tprs[profession][\"f\"] = tpr_f\n",
    "        tprs_ratio.append(tpr_m/tpr_f)\n",
    "        tprs_change[profession] = tpr_m - tpr_f\n",
    "        \n",
    "    return tprs, tprs_change, np.mean(np.abs(tprs_ratio))\n",
    "\n",
    "def get_FPR(y_pred, y_true, p2i, i2p, gender):\n",
    "    \n",
    "    scores = defaultdict(Counter)\n",
    "    prof_count_total = defaultdict(Counter)\n",
    "    \n",
    "    for y_hat, y, g in zip(y_pred, y_true, gender):\n",
    "        \n",
    "        if y != y_hat:\n",
    "            \n",
    "            scores[i2p[y]][g] += 1 # count false positives for y\n",
    "        \n",
    "        prof_count_total[i2p[y]][g] += 1 # count \"not y\"\n",
    "    \n",
    "    fprs = defaultdict(dict)\n",
    "    fprs_change = dict()\n",
    "    fprs_ratio = []\n",
    "    \n",
    "    for profession, scores_dict in scores.items():\n",
    "        \n",
    "        good_m, good_f = scores_dict[\"m\"], scores_dict[\"f\"]\n",
    "        prof_total_f = prof_count_total[profession][\"f\"]\n",
    "        prof_total_m = prof_count_total[profession][\"m\"]\n",
    "        fpr_m = good_m / prof_total_m\n",
    "        fpr_f = good_f / prof_total_f\n",
    "        \n",
    "        fprs[profession][\"m\"] = fpr_m\n",
    "        fprs[profession][\"f\"] = fpr_f\n",
    "        fprs_ratio.append(fpr_m/fpr_f)\n",
    "        fprs_change[profession] = fpr_m - fpr_f\n",
    "        \n",
    "    return fprs, fprs_change, np.mean(np.abs(fprs_ratio))    \n",
    "    \n",
    "def similarity_vs_tpr(tprs, word2vec, title, measure, prof2fem):\n",
    "    \n",
    "    professions = list(tprs.keys())\n",
    "    #\n",
    "    \"\"\" \n",
    "    sims = dict()\n",
    "    gender_direction = word2vec[\"he\"] - word2vec[\"she\"]\n",
    "    \n",
    "    for p in professions:\n",
    "        sim = word2vec.cosine_similarities(word2vec[p], [gender_direction])[0]\n",
    "        sims[p] = sim\n",
    "    \"\"\"\n",
    "    tpr_lst = [tprs[p] for p in professions]\n",
    "    sim_lst = [prof2fem[p] for p in professions]\n",
    "\n",
    "    #professions = [p.replace(\"_\", \" \") for p in professions if p in word2vec]\n",
    "    \n",
    "    plt.plot(sim_lst, tpr_lst, marker = \"o\", linestyle = \"none\")\n",
    "    plt.xlabel(\"% women\", fontsize = 13)\n",
    "    plt.ylabel(\"{}_diff_female {}\".format(measure, title), fontsize = 13)\n",
    "    for p in professions:\n",
    "        x,y = prof2fem[p], tprs[p]\n",
    "        plt.annotate(p , (x,y), size = 4, color = \"red\")\n",
    "    plt.savefig(\"{}_vs_bias_{}\".format(measure, title), dpi = 600)\n",
    "    print(\"Correlation: {}; p-value: {}\".format(*pearsonr(sim_lst, tpr_lst)))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def save_vecs_and_words(vecs, words):\n",
    "    def to_string(arr):\n",
    "        return \"\\t\".join([str(x) for x in arr])\n",
    "    \n",
    "    with open(\"vecs.txt\", \"w\") as f:\n",
    "        for v in vecs:\n",
    "            assert len(v) == 300\n",
    "            f.write(to_string(v) + \"\\n\")\n",
    "    \n",
    "    with open(\"labels.txt\", \"w\") as f:\n",
    "            f.write(\"Profession\\n\")\n",
    "            for w in words:\n",
    "                f.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_before = clf_original.predict(X_dev)\n",
    "dev_gender = [i2g[d[\"g\"]] for d in dev]\n",
    "tprs_before, tprs_change_before, mean_ratio_before = get_TPR(y_pred_before, Y_dev, p2i, i2p, dev_gender)\n",
    "similarity_vs_tpr(tprs_change_before, word2vec, \"before\", \"TPR\", prof2fem)\n",
    "\n",
    "\n",
    "y_pred_after = clf.predict(X_dev.dot(P))\n",
    "tprs, tprs_change_after, mean_ratio_after = get_TPR(y_pred_after, Y_dev, p2i, i2p, dev_gender)\n",
    "similarity_vs_tpr(tprs_change_after, word2vec, \"after\", \"TPR\", prof2fem)\n",
    "\n",
    "print(\"TPR diff ratio before: {}; after: {}\".format(mean_ratio_before, mean_ratio_after))\n",
    "\n",
    "\n",
    "fprs_before, fprs_change_before, mean_ratio_before = get_FPR(y_pred_before, Y_dev, p2i, i2p, dev_gender)\n",
    "similarity_vs_tpr(fprs_change_before, word2vec, \"before\", \"FPR\", prof2fem)\n",
    "\n",
    "\n",
    "fprs, fprs_change_after, mean_ratio_after = get_FPR(y_pred_after, Y_dev, p2i, i2p, dev_gender)\n",
    "similarity_vs_tpr(fprs_change_after, word2vec, \"after\", \"FPR\", prof2fem)\n",
    "\n",
    "print(\"TPR diff ratio before: {}; after: {}\".format(mean_ratio_before, mean_ratio_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof2fem[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.rand(X_dev.shape[0]) < 0.15\n",
    "x_dev_labels = np.array([i2p[y] for i,y in enumerate(Y_dev)])\n",
    "save_vecs_and_words(X_dev[idx], x_dev_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_by_gender(vecs, labels, title, words = None):\n",
    "\n",
    "  tsne = TSNE(n_components=2, random_state=0)\n",
    "  vecs_2d = tsne.fit_transform(vecs)\n",
    "  num_labels = len(set(labels.tolist()))\n",
    "\n",
    "  names = [\"class {}\".format(i) for i in range(num_labels)]\n",
    "  plt.figure(figsize=(6, 5))\n",
    "  colors = 'r', 'b', 'orange'\n",
    "  for i, c, label in zip(set(labels.tolist()), colors, names):\n",
    "    print(len(vecs_2d[labels == i, 0]))\n",
    "    plt.scatter(vecs_2d[labels == i, 0], vecs_2d[labels == i, 1], c=c,\n",
    "                label=label, alpha = 0.1)\n",
    "  plt.legend()\n",
    "  plt.title(title)\n",
    "  \n",
    "  if words is not None:\n",
    "        k = 60\n",
    "        for i in range(k):\n",
    "            \n",
    "            j = np.random.choice(range(len(words)))\n",
    "            label = labels[i]\n",
    "            w = words[j]\n",
    "            x,y = vecs_2d[i]\n",
    "            plt.annotate(w , (x,y), size = 10, color = \"black\" if label == 1 else \"black\")\n",
    "            \n",
    "  plt.show()\n",
    "  return vecs_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
