{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../data/embeddings\")\n",
    "sys.path.append(\"../data/embeddings/biasbios\")\n",
    "import classifier\n",
    "import svm_classifier\n",
    "import debias\n",
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname = \"../data/data.biosbias.pickle\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def count_profs_and_gender(data: List[dict]):\n",
    "    \n",
    "    counter = defaultdict(Counter)\n",
    "    for entry in data:\n",
    "        gender, prof = entry[\"gender\"], entry[\"raw_title\"]\n",
    "        counter[prof.lower()][gender.lower()] += 1\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def filter_dataset(data, topk = 10):\n",
    "    \n",
    "    filtered = []\n",
    "    counter = count_profs_and_gender(data)\n",
    "    total_counts = [(prof, counter[prof][\"f\"] + counter[prof][\"m\"]) for prof in counter.keys()]\n",
    "    profs_by_frq = sorted(total_counts, key = lambda x: -x[1])\n",
    "    topk_profs = [p[0] for p in profs_by_frq[:topk]]\n",
    "    \n",
    "    print(\"Top-k professions: {}\".format(topk_profs))\n",
    "    for d in data:\n",
    "        \n",
    "        if d[\"raw_title\"].lower() in topk_profs:\n",
    "            filtered.append(d)\n",
    "    \n",
    "    return filtered\n",
    "    \n",
    "def split_train_dev_test(data):\n",
    "    \n",
    "    g2i, i2g = {\"m\": 0, \"f\": 1}, {1: \"f\", 0: \"m\"}\n",
    "    all_profs = list(set([d[\"raw_title\"].lower() for d in data]))\n",
    "    all_words = []\n",
    "    for d in data:\n",
    "        all_words.extend(d[\"raw\"].split(\" \"))\n",
    "    \n",
    "    all_words = set(all_words)\n",
    "    all_words.add(\"<UNK>\")\n",
    "    \n",
    "    p2i = {p:i for i,p in enumerate(sorted(all_profs))}\n",
    "    i2p = {i:p for i,p in enumerate(sorted(all_profs))}\n",
    "    w2i = {w:i for i,w in enumerate(sorted(all_words))}\n",
    "    i2w = {i:w for i,w in enumerate(sorted(all_words))}\n",
    "    \n",
    "    all_data = []\n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        gender, prof = entry[\"gender\"].lower(), entry[\"raw_title\"].lower()\n",
    "        raw, start_index = entry[\"raw\"], entry[\"start_pos\"]\n",
    "        all_data.append({\"g\": g2i[gender], \"p\": p2i[prof], \"text\": raw, \"start\": start_index})\n",
    "\n",
    "\n",
    "    train_dev, test = sklearn.model_selection.train_test_split(all_data, test_size = 0.2, random_state = 0)\n",
    "    train, dev = sklearn.model_selection.train_test_split(train_dev, test_size = 0.3, random_state = 0)\n",
    "    print(\"Train size: {}; Dev size: {}; Test size: {}\".format(len(train), len(dev), len(test)))\n",
    "    return (train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k professions: ['associate professor', 'assistant professor', 'attorney', 'journalist', 'photographer', 'teacher', 'psychologist', 'physician', 'architect', 'poet', 'nurse', 'painter', 'filmmaker', 'composer', 'model', 'software engineer', 'dentist', 'surgeon', 'comedian', 'psychotherapist', 'dietitian', 'pastor', 'plastic surgeon', 'accountant', 'trial lawyer', 'orthopedic surgeon', 'chiropractor', 'trial attorney']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148816/148816 [00:00<00:00, 671866.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 83336; Dev size: 35716; Test size: 29764\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "counter = count_profs_and_gender(data)\n",
    "data = filter_dataset(data, topk = 28)\n",
    "(train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w) = split_train_dev_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input representatons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(fname = \"../data/embeddings/vecs.filtered.with_gendered.glove.txt\"):\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\n",
    "def get_embeddings_based_dataset(data: List[dict], word2vec_model):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    for entry in data:\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        bagofwords = np.sum([word2vec_model[w] for w in words if w in word2vec_model], axis = 0)\n",
    "        X.append(bagofwords)\n",
    "        Y.append(y)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "def get_BOW_based_dataset(data: List[dict], w2i):\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = True)\n",
    "    X, Y = [], []\n",
    "    data_dicts = []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        entry_dict = {w:w2i[w] if w in w2i else w2i[\"<UNK>\"] for w in words}\n",
    "        data_dicts.append(entry_dict)\n",
    "        Y.append(y)\n",
    "    \n",
    "    X = vectorizer.fit_transform(data_dicts)\n",
    "    return X,Y\n",
    "    \n",
    "def get_bert_based_dataset(data: List[dict]):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        text = text.lower()            \n",
    "        tokenized_text = tokenizer.tokenize(sentence_str)    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "              outputs = model(tokens_tensor) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83336/83336 [00:01<00:00, 42369.55it/s]\n"
     ]
    }
   ],
   "source": [
    "word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\") #load_word2vec()\n",
    "X_train, Y_train = get_BOW_based_dataset(train, w2i)\n",
    "X_dev, Y_dev = get_BOW_based_dataset(dev, w2i) \n",
    "#X_train, Y_train = get_embeddings_based_dataset(train, word2vec)\n",
    "#X_dev, Y_dev =  get_embeddings_based_dataset(dev, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = LinearSVC(verbose = 10) #LogisticRegression()\n",
    "clf = LogisticRegression(solver = \"sag\", verbose = 10, n_jobs = 4)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
