{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../data/embeddings\")\n",
    "sys.path.append(\"../data/embeddings/biasbios\")\n",
    "import classifier\n",
    "import svm_classifier\n",
    "import debias\n",
    "import gensim\n",
    "import codecs\n",
    "import json\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy.stats.stats import pearsonr\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, Perceptron, LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import copy\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname = \"../data/BIOS.pkl\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def count_profs_and_gender(data: List[dict]):\n",
    "    \n",
    "    counter = defaultdict(Counter)\n",
    "    for entry in data:\n",
    "        gender, prof = entry[\"gender\"], entry[\"raw_title\"]\n",
    "        counter[prof.lower()][gender.lower()] += 1\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def filter_dataset(data, topk = 10):\n",
    "    \n",
    "    filtered = []\n",
    "    counter = count_profs_and_gender(data)\n",
    "    total_counts = [(prof, counter[prof][\"f\"] + counter[prof][\"m\"]) for prof in counter.keys()]\n",
    "    profs_by_frq = sorted(total_counts, key = lambda x: -x[1])\n",
    "    topk_profs = [p[0] for p in profs_by_frq[:topk]]\n",
    "    \n",
    "    print(\"Top-k professions: {}\".format(topk_profs))\n",
    "    for d in data:\n",
    "        \n",
    "        if d[\"raw_title\"].lower() in topk_profs:\n",
    "            filtered.append(d)\n",
    "    \n",
    "    return filtered\n",
    "    \n",
    "def split_train_dev_test(data):\n",
    "    \n",
    "    g2i, i2g = {\"m\": 0, \"f\": 1}, {1: \"f\", 0: \"m\"}\n",
    "    all_profs = list(set([d[\"raw_title\"].lower() for d in data]))\n",
    "    all_words = []\n",
    "    for d in data:\n",
    "        all_words.extend(d[\"raw\"].split(\" \"))\n",
    "    \n",
    "    all_words = set(all_words)\n",
    "    all_words.add(\"<UNK>\")\n",
    "    \n",
    "    p2i = {p:i for i,p in enumerate(sorted(all_profs))}\n",
    "    i2p = {i:p for i,p in enumerate(sorted(all_profs))}\n",
    "    w2i = {w:i for i,w in enumerate(sorted(all_words))}\n",
    "    i2w = {i:w for i,w in enumerate(sorted(all_words))}\n",
    "    \n",
    "    all_data = []\n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        gender, prof = entry[\"gender\"].lower(), entry[\"raw_title\"].lower()\n",
    "        raw, start_index = entry[\"raw\"], entry[\"start_pos\"]\n",
    "        all_data.append({\"g\": g2i[gender], \"p\": p2i[prof], \"text\": raw, \"start\": start_index})\n",
    "\n",
    "\n",
    "    train_dev, test = sklearn.model_selection.train_test_split(all_data, test_size = 0.2, random_state = 0)\n",
    "    train, dev = sklearn.model_selection.train_test_split(train_dev, test_size = 0.3, random_state = 0)\n",
    "    print(\"Train size: {}; Dev size: {}; Test size: {}\".format(len(train), len(dev), len(test)))\n",
    "    return (train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k professions: ['associate professor', 'assistant professor', 'physician', 'attorney', 'photographer', 'journalist', 'nurse', 'teacher', 'psychologist', 'architect', 'dentist', 'painter', 'poet', 'model', 'filmmaker', 'composer', 'software engineer', 'surgeon', 'comedian', 'dietitian', 'accountant', 'psychotherapist', 'pastor', 'orthopedic surgeon', 'trial lawyer', 'chiropractor', 'plastic surgeon', 'trial attorney', 'paralegal', 'senior software engineer', 'interior designer', 'dj', 'rapper', 'certified public accountant', 'cpa', 'neurosurgeon', 'yoga teacher', 'nutritionist', 'personal trainer', 'certified personal trainer', 'yoga instructor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236263/236263 [00:00<00:00, 840322.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 132307; Dev size: 56703; Test size: 47253\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "counter = count_profs_and_gender(data)\n",
    "data = filter_dataset(data, topk = 60)\n",
    "(train, dev, test), (g2i, i2g, p2i, i2p, w2i, i2w) = split_train_dev_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'assistant professor': Counter({'f': 18061, 'm': 20401}),\n",
       "             'certified public accountant': Counter({'m': 514, 'f': 292}),\n",
       "             'journalist': Counter({'f': 6514, 'm': 6733}),\n",
       "             'architect': Counter({'m': 4807, 'f': 1392}),\n",
       "             'photographer': Counter({'m': 10230, 'f': 5301}),\n",
       "             'psychologist': Counter({'m': 3605, 'f': 5350}),\n",
       "             'teacher': Counter({'f': 5658, 'm': 3982}),\n",
       "             'nurse': Counter({'f': 10303, 'm': 1022}),\n",
       "             'associate professor': Counter({'f': 16243, 'm': 22294}),\n",
       "             'attorney': Counter({'f': 6857, 'm': 10430}),\n",
       "             'software engineer': Counter({'m': 2896, 'f': 520}),\n",
       "             'trial attorney': Counter({'m': 753, 'f': 241}),\n",
       "             'painter': Counter({'f': 2149, 'm': 2548}),\n",
       "             'physician': Counter({'m': 8291, 'f': 11752}),\n",
       "             'chiropractor': Counter({'m': 897, 'f': 365}),\n",
       "             'personal trainer': Counter({'m': 240, 'f': 156}),\n",
       "             'surgeon': Counter({'m': 2395, 'f': 524}),\n",
       "             'filmmaker': Counter({'m': 2896, 'f': 1444}),\n",
       "             'dietitian': Counter({'f': 1684, 'm': 68}),\n",
       "             'dentist': Counter({'m': 3652, 'f': 1805}),\n",
       "             'dj': Counter({'m': 727, 'f': 122}),\n",
       "             'model': Counter({'f': 3751, 'm': 757}),\n",
       "             'composer': Counter({'m': 3106, 'f': 568}),\n",
       "             'psychotherapist': Counter({'m': 504, 'f': 1166}),\n",
       "             'poet': Counter({'f': 2248, 'm': 2314}),\n",
       "             'trial lawyer': Counter({'m': 1090, 'f': 199}),\n",
       "             'plastic surgeon': Counter({'m': 1066, 'f': 195}),\n",
       "             'comedian': Counter({'m': 1433, 'f': 370}),\n",
       "             'yoga instructor': Counter({'f': 323, 'm': 48}),\n",
       "             'interior designer': Counter({'f': 696, 'm': 166}),\n",
       "             'senior software engineer': Counter({'f': 107, 'm': 804}),\n",
       "             'pastor': Counter({'m': 1201, 'f': 374}),\n",
       "             'rapper': Counter({'m': 768, 'f': 75}),\n",
       "             'yoga teacher': Counter({'f': 512, 'm': 106}),\n",
       "             'orthopedic surgeon': Counter({'m': 1315, 'f': 81}),\n",
       "             'accountant': Counter({'m': 1131, 'f': 583}),\n",
       "             'neurosurgeon': Counter({'m': 570, 'f': 51}),\n",
       "             'cpa': Counter({'m': 363, 'f': 273}),\n",
       "             'certified personal trainer': Counter({'m': 190, 'f': 205}),\n",
       "             'nutritionist': Counter({'m': 79, 'f': 439}),\n",
       "             'paralegal': Counter({'f': 787, 'm': 135})})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input representatons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(fname = \"../data/embeddings/vecs.filtered.with_gendered.glove.txt\"):\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=False)\n",
    "    vecs = model.vectors\n",
    "    words = list(model.vocab.keys())\n",
    "    return model, vecs, words\n",
    "\n",
    "def get_embeddings_based_dataset(data: List[dict], word2vec_model):\n",
    "    \n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        #print(text)\n",
    "        #print(\"----------\")\n",
    "        #print(\" \".join(words))\n",
    "        #print(\"=====================\")\n",
    "        bagofwords = np.sum([word2vec_model[w] if w in word2vec_model else word2vec_model['unk'] for w in words], axis = 0)\n",
    "        X.append(bagofwords)\n",
    "        Y.append(y)\n",
    "        total += len(words)\n",
    "        unk += len([w for w in words if w not in word2vec_model])\n",
    "    \n",
    "    print(\"% unknown: {}\".format(unk/total))\n",
    "    return X,Y\n",
    "\n",
    "def get_BOW_based_dataset(data: List[dict], w2i):\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = True)\n",
    "    X, Y = [], []\n",
    "    unk, total = 0., 0.\n",
    "    data_dicts = []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        text = text.lower()\n",
    "        words = text[start + 1:].split(\" \")\n",
    "        entry_dict = {w:w2i[w] if w in w2i else w2i[\"<UNK>\"] for w in words}\n",
    "        data_dicts.append(entry_dict)\n",
    "        Y.append(y)\n",
    "        \n",
    "        total += len(words)\n",
    "        unk += len([w for w in words if w not in w2i])\n",
    "    \n",
    "    print(\"% unknown: {}\".format(unk/total))\n",
    "    X = vectorizer.fit_transform(data_dicts)\n",
    "    return X,Y\n",
    "    \n",
    "def get_bert_based_dataset(data: List[dict]):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    X, Y = [], []\n",
    "    \n",
    "    for entry in tqdm.tqdm(data, total = len(data)):\n",
    "        text, start, y = entry[\"text\"], entry[\"start\"], entry[\"p\"]\n",
    "        #text = text.lower()            \n",
    "        tokenized_text = tokenizer.tokenize(sentence_str)    \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "              outputs = model(tokens_tensor) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132307/132307 [00:22<00:00, 5880.23it/s]\n",
      "  2%|▏         | 1186/56703 [00:00<00:09, 5925.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% unknown: 0.10565114291350908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56703/56703 [00:09<00:00, 5702.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% unknown: 0.10572557408218906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec, vecs, words = load_word_vectors(\"../data/embeddings/wiki-news-300d-1M-subword.vec\") #load_word2vec()\n",
    "#word2vec.init_sims(replace = True)\n",
    "#X_train, Y_train = get_BOW_based_dataset(train, w2i)\n",
    "#X_dev, Y_dev = get_BOW_based_dataset(dev, w2i) \n",
    "X_train, Y_train = get_embeddings_based_dataset(train, word2vec)\n",
    "X_dev, Y_dev =  get_embeddings_based_dataset(dev, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1222 seconds\n",
      "max_iter reached after 1231 seconds\n",
      "max_iter reached after 1231 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done   3 out of  41 | elapsed: 20.5min remaining: 260.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1232 seconds\n",
      "max_iter reached after 1233 seconds\n",
      "max_iter reached after 1233 seconds\n",
      "max_iter reached after 1233 seconds\n",
      "max_iter reached after 1234 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done   8 out of  41 | elapsed: 20.6min remaining: 84.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1234 seconds\n",
      "max_iter reached after 1235 seconds\n",
      "max_iter reached after 1235 seconds\n",
      "max_iter reached after 1235 seconds\n",
      "max_iter reached after 1236 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  13 out of  41 | elapsed: 20.6min remaining: 44.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1238 seconds\n",
      "max_iter reached after 1238 seconds\n",
      "max_iter reached after 1238 seconds\n",
      "max_iter reached after 1238 seconds\n",
      "max_iter reached after 1239 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  18 out of  41 | elapsed: 20.6min remaining: 26.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1239 seconds\n",
      "max_iter reached after 1239 seconds\n",
      "max_iter reached after 1239 seconds\n",
      "max_iter reached after 1239 seconds\n",
      "max_iter reached after 1240 seconds\n",
      "max_iter reached after 1240 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  23 out of  41 | elapsed: 20.7min remaining: 16.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1240 seconds\n",
      "max_iter reached after 1241 seconds\n",
      "max_iter reached after 1241 seconds\n",
      "max_iter reached after 1241 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  28 out of  41 | elapsed: 20.7min remaining:  9.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 1242 seconds\n",
      "max_iter reached after 1243 seconds\n",
      "max_iter reached after 1246 seconds\n",
      "max_iter reached after 1248 seconds\n",
      "max_iter reached after 322 seconds\n",
      "max_iter reached after 334 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  33 out of  41 | elapsed: 25.9min remaining:  6.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 326 seconds\n",
      "max_iter reached after 325 seconds\n",
      "max_iter reached after 324 seconds\n",
      "max_iter reached after 327 seconds\n",
      "max_iter reached after 325 seconds\n",
      "max_iter reached after 325 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  38 out of  41 | elapsed: 26.0min remaining:  2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 326 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Done  41 out of  41 | elapsed: 26.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5980283230164188\n"
     ]
    }
   ],
   "source": [
    "#clf = LinearSVC(verbose = 10) #LogisticRegression()\n",
    "clf = LogisticRegression(solver = \"sag\", multi_class = 'ovr', verbose = 10, n_jobs = 32)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6056595644977212\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf.predict(X_dev)\n",
    "cm = sklearn.metrics.confusion_matrix(Y_dev,y_hat)\n",
    "labels = [i2p[i] for i in range(len(i2p))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=0.3)#for label size\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.savefig(\"confusion.png\", dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
